{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaab0007-36a3-40df-b409-534e9e848735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo-mirror.mos.ru/repository/pypi-proxy/simple\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.92)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (1.13.1+cu116)\n",
      "Collecting regex\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/regex/2023.5.5/regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.92 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.29.92)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.92->boto3) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.92->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.92->boto3) (1.16.0)\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2023.5.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 torch regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb6c71d-9ac9-4b5f-b364-418d19379e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuditdDataloader:\n",
    "    def __init__(self, minio_config: dict, bucket: str, key: str, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "        s3 = boto3.resource(**minio_config)\n",
    "        dataobj = s3.Object(bucket_name=bucket, key=key)\n",
    "        buffer = io.BytesIO()\n",
    "        dataobj.download_fileobj(buffer)\n",
    "        self.parquet_file = pq.ParquetFile(buffer)\n",
    "        buffer.flush()\n",
    "        \n",
    "\n",
    "    def loader(self):\n",
    "        return self.parquet_file.iter_batches(batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c03390b-146a-4255-a8eb-f1170a1572f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "import boto3\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "minio_config =  {'service_name': 's3',\n",
    "                  'endpoint_url': 'http://data-minio.udrvs-prod-aiplatform-tools-minio.svc.cluster.local:9000',\n",
    "                  'aws_access_key_id': '6q5iZHIIvA62s8tD',\n",
    "                  'aws_secret_access_key': 'maiHOzSqdsY5OOOTxvYVl3L2lgTXBVLO',\n",
    "                  'config': Config(signature_version='s3v4'),\n",
    "                  'region_name': 'us-east-1'}\n",
    "\n",
    "bucket = 'udrvs-prod-aiplatform-data-dud-ds'\n",
    "key = 'data/auditd/final_dataset.parquet'\n",
    "\n",
    "audit_data_loader = AuditdDataloader(minio_config=minio_config, bucket=bucket, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72563c95-1516-4566-a6da-d304b8d23bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append(\"~/\")\n",
    "\n",
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1):\n",
    "        print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "        for line in texts:\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = str(line).replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a25570-58e7-4eb3-bf54-126f0e19c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "X_train = []\n",
    "\n",
    "for i, X in enumerate(audit_data_loader.loader()):\n",
    "    X_train.append(X.to_pandas().auditd.values)\n",
    "    if i == 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7404be43-fb17-4ad3-a7f2-ec688a7ebbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab\n",
      "VOCAB SIZE: 372441\n",
      "save vocab in output/vocab\n"
     ]
    }
   ],
   "source": [
    "vocab = WordVocab(X_train, max_size=None, min_freq=1)\n",
    "print(\"VOCAB SIZE:\", len(vocab))\n",
    "print(\"save vocab in\", 'output/vocab')\n",
    "vocab.save_vocab('/home/maslovapo_user/output/vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4450101-2d02-4ded-a3c3-2c23fe2e497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6fe7e1-8127-47c9-8eeb-22cb59cdf67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5adf8ca-86de-4592-a31a-4b6c2495545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0)\n",
    "        \n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Linear(1, embed_size)\n",
    "\n",
    "    def forward(self, time_interval):\n",
    "        return self.time_embed(time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00edb80-f609-421b-a381-8392df044b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95156b3-0434-4033-ada3-ea3fa31651a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "108e802f-0b8d-4ecd-88c2-2dea82ad9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "   \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d37e02b3-7be2-4ad3-954f-89fac039a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "   \n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a25135-66d1-4414-969f-e106845aa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "   \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81444073-610e-4f1b-a962-db254b017de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ccc534-afe5-4e66-b7fd-d23df9f662c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "664b9925-a330-4308-9cef-b0aee0b34621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1, is_logkey=True, is_time=False):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim, max_len=max_len)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.time_embed = TimeEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "        self.is_logkey = is_logkey\n",
    "        self.is_time = is_time\n",
    "\n",
    "    def forward(self, sequence, segment_label=None, time_info=None):\n",
    "        x = self.position(sequence)\n",
    "        # if self.is_logkey:\n",
    "        x = x + self.token(sequence)\n",
    "        if segment_label is not None:\n",
    "            x = x + self.segment(segment_label)\n",
    "        if self.is_time:\n",
    "            x = x + self.time_embed(time_info)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d8cfa37-a413-41a9-8c72-534ee0669dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, max_len=512, hidden=768, n_layers=12, attn_heads=12, dropout=0.1, is_logkey=True, is_time=False):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = hidden * 2\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden, max_len=max_len, is_logkey=is_logkey, is_time=is_time)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 2, dropout) for _ in range(n_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, x, segment_info=None, time_info=None):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "       \n",
    "        mask = (x>0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info, time_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15ab3754-3264-44ee-a154-18063407d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BERTLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)\n",
    "\n",
    "\n",
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37d48a45-d8b4-437b-87a8-755b8c872503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BERTLog(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mask_lm = MaskedLogModel(self.bert.hidden, vocab_size)\n",
    "        # self.fnn_cls = LinearCLS(self.bert.hidden)\n",
    "        #self.cls_lm = LogClassifier(self.bert.hidden)\n",
    "        self.result = {\"logkey_output\": None, \"cls_output\": None, \"cls_fnn_output\": None}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "\n",
    "        self.result[\"logkey_output\"] = self.mask_lm(x)\n",
    "       \n",
    "        # self.result[\"cls_output\"] = x.float().mean(axis=1) #x[:, 0]\n",
    "        self.result[\"cls_output\"] = x[:, 0]\n",
    "        # self.result[\"cls_output\"] = self.fnn_cls(x[:, 0])\n",
    "\n",
    "        # print(self.result[\"cls_fnn_output\"].shape)\n",
    "\n",
    "        return self.result\n",
    "\n",
    "class MaskedLogModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "\n",
    "class TimeLogModel(nn.Module):\n",
    "    def __init__(self, hidden, time_size=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, time_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class LogClassifier(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, hidden)\n",
    "\n",
    "    def forward(self, cls):\n",
    "        return self.linear(cls)\n",
    "\n",
    "class LinearCLS(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f92c58f-375c-4561-b63f-1402ca240613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ScheduledOptim():\n",
    "    \n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28969e61-933e-43be-838c-a2a1ec13d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader\n",
    "\n",
    "class BERTTrainer:\n",
    "    \"\"\"\n",
    "    BERTTrainer make the pretrained BERT model with two LM training method.\n",
    "\n",
    "        1. Masked Language Model : 3.3.1 Task #1: Masked LM\n",
    "        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction\n",
    "\n",
    "    please check the details on README.md with simple example.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size: int,\n",
    "                 train_dataloader: data_loader, \n",
    "                 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000,\n",
    "                 with_cuda: bool = True, cuda_devices=None, log_freq: int = 10, is_logkey=True, is_time=False,\n",
    "                 hypersphere_loss=False):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param vocab_size: total word vocab size\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param valid_dataloader: valid dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        # Initialize the BERT Language Model, with BERT model\n",
    "        self.model = BERTLog(bert, vocab_size).to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        # if with_cuda and torch.cuda.device_count() > 1:\n",
    "        #     print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "        #     self.model = nn.DataParallel(self.model, device_ids=cuda_devices)\n",
    "\n",
    "        # Setting the train data loader\n",
    "        self.train_data = train_dataloader\n",
    "        \n",
    "\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optim = None\n",
    "        self.optim_schedule = None\n",
    "        self.init_optimizer()\n",
    "\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "        self.time_criterion = nn.MSELoss()\n",
    "        self.hyper_criterion = nn.MSELoss()\n",
    "\n",
    "        # deep SVDD hyperparameters\n",
    "        self.hypersphere_loss = hypersphere_loss\n",
    "        self.radius = 0\n",
    "        self.hyper_center = None\n",
    "        self.nu = 0.25\n",
    "        # self.objective = \"soft-boundary\"\n",
    "        self.objective = None\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        self.log = {\n",
    "            \"train\": {key: []\n",
    "                      for key in [\"epoch\", \"lr\", \"time\", \"loss\"]},\n",
    "            \"valid\": {key: []\n",
    "                      for key in [\"epoch\", \"lr\", \"time\", \"loss\"]}\n",
    "        }\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "        self.is_logkey = is_logkey\n",
    "        self.is_time = is_time\n",
    "\n",
    "    def init_optimizer(self):\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=self.lr, betas=self.betas, weight_decay=self.weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=self.warmup_steps)\n",
    "\n",
    "    def train(self, epoch):\n",
    "        return self.iteration(epoch, self.train_data, start_train=True)\n",
    "\n",
    "    def valid(self, epoch):\n",
    "        return self.iteration(epoch, start_train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, start_train):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or validing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every peoch\n",
    "\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or valid\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if start_train else \"valid\"\n",
    "\n",
    "        lr = self.optim.state_dict()['param_groups'][0]['lr']\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        self.log[str_code]['lr'].append(lr)\n",
    "        self.log[str_code]['time'].append(start)\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        totol_length = len(data_loader)\n",
    "        # data_iter = tqdm.tqdm(enumerate(data_loader), total=totol_length)\n",
    "        data_iter = enumerate(data_loader)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_logkey_loss = 0.0\n",
    "        total_hyper_loss = 0.0\n",
    "\n",
    "        total_dist = []\n",
    "        \n",
    "        for t in data_iter:\n",
    "            data = t[1]\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            result = self.model.forward(data[\"bert_input\"])\n",
    "            mask_lm_output = result[\"logkey_output\"]\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word ignore_index = 0 to ignore unmasked tokens\n",
    "            mask_loss = torch.tensor(0) if not self.is_logkey else self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "            total_logkey_loss += mask_loss.item()\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = mask_loss\n",
    "\n",
    "            # hypersphere loss\n",
    "            if self.hypersphere_loss:\n",
    "                # version 1.0\n",
    "                # hyper_loss = self.hyper_criterion(result[\"cls_fnn_output\"].squeeze(), self.hyper_center.expand(data[\"bert_input\"].shape[0],-1))\n",
    "                hyper_loss = self.hyper_criterion(result[\"cls_output\"].squeeze(), self.hyper_center.expand(data[\"bert_input\"].shape[0], -1))\n",
    "\n",
    "                # version 2.0 https://github.com/lukasruff/Deep-SVDD-PyTorch/blob/master/src/optim/deepSVDD_trainer.py\n",
    "                dist = torch.sum((result[\"cls_output\"] - self.hyper_center) ** 2, dim=1)\n",
    "                total_dist += dist.cpu().tolist()\n",
    "\n",
    "                # if self.objective == 'soft-boundary':\n",
    "                #     scores = dist - self.radius ** 2\n",
    "                #     hyper_loss = torch.sqrt(self.radius ** 2 + (1 / self.nu) * torch.mean(torch.max(torch.zeros_like(scores), scores)))\n",
    "                # else:\n",
    "                #     hyper_loss = torch.sqrt(torch.mean(dist))\n",
    "\n",
    "                # # add radius and center to training\n",
    "                # self.radius = self.get_radius(dist, self.nu)\n",
    "                # self.hyper_center = torch.mean(result[\"cls_output\"], dim=0)\n",
    "\n",
    "                total_hyper_loss += hyper_loss.item()\n",
    "\n",
    "                # with deepsvdd loss\n",
    "                loss = loss + 0.1 * hyper_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if start_train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "        avg_loss = total_loss / totol_length\n",
    "        self.log[str_code]['epoch'].append(epoch)\n",
    "        self.log[str_code]['loss'].append(avg_loss)\n",
    "        print(\"Epoch: {} | phase: {}, loss={}\".format(epoch, str_code, avg_loss))\n",
    "        print(f\"logkey loss: {total_logkey_loss/totol_length}, hyper loss: {total_hyper_loss/totol_length}\\n\")\n",
    "\n",
    "        return avg_loss, total_dist\n",
    "\n",
    "    def save_log(self, save_dir, surfix_log):\n",
    "        try:\n",
    "            for key, values in self.log.items():\n",
    "                pd.DataFrame(values).to_csv(save_dir + key + f\"_{surfix_log}.csv\",\n",
    "                                            index=False)\n",
    "            print(\"Log saved\")\n",
    "        except:\n",
    "            print(\"Failed to save logs\")\n",
    "\n",
    "    def save(self, save_dir=\"\"):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        torch.save(self.model, save_dir)\n",
    "        # self.bert.to(self.device)\n",
    "        print(\" Model Saved on:\", save_dir)\n",
    "        return save_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def get_radius(dist: list, nu: float):\n",
    "        \"\"\"Optimally solve for radius R via the (1-nu)-quantile of distances.\"\"\"\n",
    "        return np.quantile(np.sqrt(dist), 1 - nu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eead418-cd91-4415-80c9-211a95dcabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def generate_pairs(line, window_size):\n",
    "    line = np.array(line)\n",
    "    line = line[:, 0]\n",
    "\n",
    "    seqs = []\n",
    "    for i in range(0, len(line), window_size):\n",
    "        seq = line[i:i + window_size]\n",
    "        seqs.append(seq)\n",
    "    seqs += []\n",
    "    seq_pairs = []\n",
    "    for i in range(1, len(seqs)):\n",
    "        seq_pairs.append([seqs[i - 1], seqs[i]])\n",
    "    return seqs\n",
    "\n",
    "\n",
    "def fixed_window(line, window_size, adaptive_window, seq_len=None, min_len=0):\n",
    "    line = [ln.split(\",\") for ln in line.split()]\n",
    "\n",
    "    # filter the line/session shorter than 10\n",
    "    if len(line) < min_len:\n",
    "        return [], []\n",
    "\n",
    "    # max seq len\n",
    "    if seq_len is not None:\n",
    "        line = line[:seq_len]\n",
    "\n",
    "    if adaptive_window:\n",
    "        window_size = len(line)\n",
    "\n",
    "    line = np.array(line, dtype=object)\n",
    "    line = line.squeeze()\n",
    "        # if time duration doesn't exist, then create a zero array for time\n",
    "    tim = np.zeros(line.shape)\n",
    "\n",
    "    logkey_seqs = []\n",
    "    time_seq = []\n",
    "    for i in range(0, len(line), window_size):\n",
    "        logkey_seqs.append(line[i:i + window_size])\n",
    "        time_seq.append(tim[i:i + window_size])\n",
    "\n",
    "    return logkey_seqs, time_seq\n",
    "\n",
    "def generate_train(data_path, window_size=20, adaptive_window=True,\n",
    "                         sample_ratio=1, valid_size=0, output_path=None,\n",
    "                         scale=None, scale_path=None, seq_len=None, min_len=0):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data_iter = f.readlines()\n",
    "\n",
    "    num_session = int(len(data_iter) * sample_ratio)\n",
    "    # only even number of samples, or drop_last=True in DataLoader API\n",
    "    # coz in parallel computing in CUDA, odd number of samples reports issue when merging the result\n",
    "    # num_session += num_session % 2\n",
    "\n",
    "    test_size = int(min(num_session, len(data_iter)) * valid_size)\n",
    "    # only even number of samples\n",
    "    # test_size += test_size % 2\n",
    "\n",
    "    print(\"train size \", int(num_session))\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    logkey_seq_pairs = []\n",
    "    time_seq_pairs = []\n",
    "    session = 0\n",
    "    for line in data_iter:\n",
    "        if session >= num_session:\n",
    "            break\n",
    "        session += 1\n",
    "\n",
    "        logkeys, times = fixed_window(line, window_size, adaptive_window, seq_len, min_len)\n",
    "        logkey_seq_pairs += logkeys\n",
    "        time_seq_pairs += times\n",
    "\n",
    "    logkey_seq_pairs = np.array(logkey_seq_pairs, dtype=object)\n",
    "    time_seq_pairs = np.array(time_seq_pairs, dtype=object)\n",
    "\n",
    "    logkey_trainset, time_trainset = logkey_seq_pairs,time_seq_pairs\n",
    "                                                                              \n",
    "\n",
    "    # sort seq_pairs by seq len\n",
    "    train_len = list(map(len, logkey_trainset))\n",
    "   \n",
    "    train_sort_index = np.argsort(-1 * np.array(train_len))\n",
    "    \n",
    "    logkey_trainset = logkey_trainset[train_sort_index]\n",
    "   \n",
    "    time_trainset = time_trainset[train_sort_index]\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(\"Num of train seqs\", len(logkey_trainset))\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    return logkey_trainset, time_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3442381f-d242-429d-9ef9-317f29fd4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(options, filename):\n",
    "    with open(filename, \"w+\") as f:\n",
    "        for key in options.keys():\n",
    "            f.write(\"{}: {}\\n\".format(key, options[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0caa064f-4c74-448f-9b2e-1b4ea342f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, log_corpus, vocab, seq_len, corpus_lines=None, encoding=\"utf-8\", on_memory=True, predict_mode=False, mask_ratio=0.15):\n",
    "        \"\"\"\n",
    "\n",
    "        :param corpus: log sessions/line\n",
    "        :param vocab: log events collection including pad, ukn ...\n",
    "        :param seq_len: max sequence length\n",
    "        :param corpus_lines: number of log sessions\n",
    "        :param encoding:\n",
    "        :param on_memory:\n",
    "        :param predict_mode: if predict\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.on_memory = on_memory\n",
    "        self.encoding = encoding\n",
    "\n",
    "        self.predict_mode = predict_mode\n",
    "        self.log_corpus = log_corpus\n",
    "        self.corpus_lines = len(log_corpus)\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k = self.log_corpus[idx]\n",
    "\n",
    "        k_masked, k_label, = self.random_item(k)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        k = [self.vocab.sos_index] + k_masked\n",
    "        k_label = [self.vocab.pad_index] + k_label\n",
    "        # k_label = [self.vocab.sos_index] + k_label\n",
    "\n",
    "        \n",
    "        return k, k_label, \n",
    "\n",
    "    def random_item(self, k):\n",
    "        tokens = list(k)\n",
    "        output_label = []\n",
    "\n",
    "       \n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            \n",
    "            prob = random.random()\n",
    "            # replace 15% of tokens in a sequence to a masked token\n",
    "            if prob < self.mask_ratio:\n",
    "                # raise AttributeError(\"no mask in visualization\")\n",
    "\n",
    "                if self.predict_mode:\n",
    "                    tokens[i] = self.vocab.mask_index\n",
    "                    output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "                prob /= self.mask_ratio\n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = self.vocab.mask_index\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = random.randrange(len(self.vocab))\n",
    "\n",
    "                # 10% randomly change token to current token\n",
    "                else:\n",
    "                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "\n",
    "                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "\n",
    "                \n",
    "\n",
    "            else:\n",
    "                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "                output_label.append(0)\n",
    "                \n",
    "        return tokens, output_label, \n",
    "\n",
    "    def collate_fn(self, batch, percentile=100, dynamical_pad=True):\n",
    "        lens = [len(seq[0]) for seq in batch]\n",
    "\n",
    "        # find the max len in each batch\n",
    "        if dynamical_pad:\n",
    "            # dynamical padding\n",
    "            seq_len = int(np.percentile(lens, percentile))\n",
    "            if self.seq_len is not None:\n",
    "                seq_len = min(seq_len, self.seq_len)\n",
    "        else:\n",
    "            # fixed length padding\n",
    "            seq_len = self.seq_len\n",
    "\n",
    "        output =  defaultdict(list)\n",
    "        for seq in batch:\n",
    "            bert_input = seq[0][:seq_len]\n",
    "            bert_label = seq[1][:seq_len]\n",
    "            \n",
    "\n",
    "            padding = [self.vocab.pad_index for _ in range(seq_len - len(bert_input))]\n",
    "            bert_input.extend(padding), bert_label.extend(padding)\n",
    "\n",
    "           \n",
    "            output[\"bert_input\"].append(bert_input)\n",
    "            output[\"bert_label\"].append(bert_label)\n",
    "            #output[\"bert_input\"] = bert_input\n",
    "            #output[\"bert_label\"] = bert_label\n",
    "            \n",
    "        output[\"bert_input\"] = torch.tensor(output[\"bert_input\"], dtype=torch.long)\n",
    "        output[\"bert_label\"] = torch.tensor(output[\"bert_label\"], dtype=torch.long)\n",
    "        \n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30d19abd-a4c5-4eb7-8c22-090e02326dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, options):\n",
    "        self.device = options[\"device\"]\n",
    "        self.model_dir = options[\"model_dir\"]\n",
    "        self.model_path = options[\"model_path\"]\n",
    "        self.vocab_path = options[\"vocab_path\"]\n",
    "        self.output_path = options[\"output_dir\"]\n",
    "        self.window_size = options[\"window_size\"]\n",
    "        self.adaptive_window = options[\"adaptive_window\"]\n",
    "        self.sample_ratio = options[\"train_ratio\"]\n",
    "        self.valid_ratio = options[\"valid_ratio\"]\n",
    "        self.seq_len = options[\"seq_len\"]\n",
    "        self.max_len = options[\"max_len\"]\n",
    "        self.corpus_lines = options[\"corpus_lines\"]\n",
    "        self.on_memory = options[\"on_memory\"]\n",
    "        self.batch_size = options[\"batch_size\"]\n",
    "        self.num_workers = options[\"num_workers\"]\n",
    "        self.lr = options[\"lr\"]\n",
    "        self.adam_beta1 = options[\"adam_beta1\"]\n",
    "        self.adam_beta2 = options[\"adam_beta2\"]\n",
    "        self.adam_weight_decay = options[\"adam_weight_decay\"]\n",
    "        self.with_cuda = options[\"with_cuda\"]\n",
    "        self.cuda_devices = options[\"cuda_devices\"]\n",
    "        self.log_freq = options[\"log_freq\"]\n",
    "        self.epochs = options[\"epochs\"]\n",
    "        self.hidden = options[\"hidden\"]\n",
    "        self.layers = options[\"layers\"]\n",
    "        self.attn_heads = options[\"attn_heads\"]\n",
    "        self.is_logkey = options[\"is_logkey\"]\n",
    "        self.is_time = options[\"is_time\"]\n",
    "        self.scale = options[\"scale\"]\n",
    "        self.scale_path = options[\"scale_path\"]\n",
    "        self.n_epochs_stop = options[\"n_epochs_stop\"]\n",
    "        self.hypersphere_loss = options[\"hypersphere_loss\"]\n",
    "        self.mask_ratio = options[\"mask_ratio\"]\n",
    "        self.min_len = options['min_len']\n",
    "#        self.train_data_loader = audit_data_loader\n",
    "\n",
    "        print(\"Save options parameters\")\n",
    "        save_parameters(options, self.model_dir + \"parameters.txt\")\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        print(\"Loading vocab\", self.vocab_path)\n",
    "        vocab = WordVocab.load_vocab(self.vocab_path)\n",
    "        print(\"vocab Size: \", len(vocab))\n",
    "\n",
    "        print(\"\\nLoading Train Dataset\")\n",
    "        logkey_train = X_train\n",
    "\n",
    "        train_dataset = LogDataset(logkey_train, vocab, seq_len=self.seq_len,\n",
    "                                    corpus_lines=self.corpus_lines, on_memory=self.on_memory, mask_ratio=self.mask_ratio)\n",
    "        \n",
    "        print(\"Creating Dataloader\")\n",
    "        self.train_data_loader = DataLoader(train_dataset, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                                      collate_fn=train_dataset.collate_fn, drop_last=True)\n",
    "        \n",
    "        del train_dataset\n",
    "        del logkey_train\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Building BERT model\")\n",
    "        bert = BERT(len(vocab), max_len=self.max_len, hidden=self.hidden, n_layers=self.layers, attn_heads=self.attn_heads,\n",
    "                    is_logkey=self.is_logkey, is_time=self.is_time)\n",
    "\n",
    "        print(\"Creating BERT Trainer\")\n",
    "        self.trainer = BERTTrainer(bert, len(vocab), train_dataloader=self.train_data_loader,\n",
    "                              lr=self.lr, betas=(self.adam_beta1, self.adam_beta2), weight_decay=self.adam_weight_decay,\n",
    "                              with_cuda=self.with_cuda, cuda_devices=self.cuda_devices, log_freq=self.log_freq,\n",
    "                              is_logkey=self.is_logkey, is_time=self.is_time,\n",
    "                              hypersphere_loss=self.hypersphere_loss)\n",
    "\n",
    "        self.start_iteration(surfix_log=\"log2\")\n",
    "        self.trainer.save(self.model_path)\n",
    "        \n",
    "\n",
    "       \n",
    "    def start_iteration(self, surfix_log):\n",
    "        print(\"Training Start\")\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        # best_center = None\n",
    "        # best_radius = 0\n",
    "        # total_dist = None\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"\\n\")\n",
    "            if self.hypersphere_loss:\n",
    "                center = self.calculate_center([self.train_data_loader])\n",
    "                self.trainer.hyper_center = center\n",
    "\n",
    "            _, train_dist = self.trainer.train(epoch)\n",
    "            #avg_loss, valid_dist = self.trainer.valid(epoch)\n",
    "            self.trainer.save_log(self.model_dir, surfix_log)\n",
    "\n",
    "            if self.hypersphere_loss:\n",
    "                self.trainer.radius = self.trainer.get_radius(train_dist, self.trainer.nu)\n",
    "            \n",
    "            # save model after 10 warm up epochs\n",
    "            #if avg_loss < best_loss:\n",
    "            #    best_loss = avg_loss\n",
    "            #    self.trainer.save(self.model_path)\n",
    "            #    epochs_no_improve = 0\n",
    "\n",
    "             #   if epoch > 10 and self.hypersphere_loss:\n",
    "             #       best_center = self.trainer.hyper_center\n",
    "             #       best_radius = self.trainer.radius\n",
    "             #      total_dist = train_dist\n",
    "\n",
    "                    #if best_center is None:\n",
    "                      #  raise TypeError(\"center is None\")\n",
    "\n",
    "                    #print(\"best radius\", best_radius)\n",
    "                   # best_center_path = self.model_dir + \"best_center.pt\"\n",
    "                    #print(\"Save best center\", best_center_path)\n",
    "                   # torch.save({\"center\": best_center, \"radius\": best_radius}, best_center_path)\n",
    "\n",
    "                   # total_dist_path = self.model_dir + \"best_total_dist.pt\"\n",
    "                   # print(\"save total dist: \", total_dist_path)\n",
    "                   # torch.save(total_dist, total_dist_path)\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve == self.n_epochs_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def calculate_center(self, data_loader_list):\n",
    "        print(\"start calculate center\")\n",
    "        # model = torch.load(self.model_path)\n",
    "        # model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = 0\n",
    "            total_samples = 0\n",
    "            for data_loader in data_loader_list:\n",
    "                totol_length = len(data_loader)\n",
    "                data_iter = enumerate(data_loader)\n",
    "                for t in data_iter:\n",
    "                    data = t[1]\n",
    "                    data = {key: value.to(self.device) for key, value in data.items()}\n",
    "                    result = self.trainer.model.forward(data[\"bert_input\"])\n",
    "                    cls_output = result[\"cls_output\"]\n",
    "\n",
    "                    outputs += torch.sum(cls_output.detach().clone(), dim=0)\n",
    "                    total_samples += cls_output.size(0)\n",
    "\n",
    "        center = outputs / total_samples\n",
    "\n",
    "        return center\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76a6e2a9-065f-4fff-87b3-12c497c52868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "features logkey:False time: False\n",
      "\n",
      "mask ratio 0.65\n",
      "Save options parameters\n",
      "Loading vocab /home/maslovapo_user/output/vocab\n",
      "vocab Size:  372441\n",
      "\n",
      "Loading Train Dataset\n",
      "Creating Dataloader\n",
      "Building BERT model\n",
      "Creating BERT Trainer\n",
      "Total Parameters: 193171929\n",
      "Training Start\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 0 | phase: train, loss=0.09790774921958263\n",
      "logkey loss: 0.0, hyper loss: 0.9790774772946651\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 1 | phase: train, loss=0.05412411177530885\n",
      "logkey loss: 0.0, hyper loss: 0.5412411094667056\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 2 | phase: train, loss=0.03401611021791513\n",
      "logkey loss: 0.0, hyper loss: 0.34016109592257404\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 3 | phase: train, loss=0.020048838559514247\n",
      "logkey loss: 0.0, hyper loss: 0.2004883815176212\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 4 | phase: train, loss=0.012393177048159907\n",
      "logkey loss: 0.0, hyper loss: 0.12393176870850417\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 5 | phase: train, loss=0.008282941645190407\n",
      "logkey loss: 0.0, hyper loss: 0.08282941448478362\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 6 | phase: train, loss=0.005788588688935703\n",
      "logkey loss: 0.0, hyper loss: 0.05788588607444977\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 7 | phase: train, loss=0.004122783002318242\n",
      "logkey loss: 0.0, hyper loss: 0.04122782921275267\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 8 | phase: train, loss=0.002992719479758913\n",
      "logkey loss: 0.0, hyper loss: 0.029927194369240448\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 9 | phase: train, loss=0.0022932371200742917\n",
      "logkey loss: 0.0, hyper loss: 0.022932370718664084\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 10 | phase: train, loss=0.0017648455459260358\n",
      "logkey loss: 0.0, hyper loss: 0.017648455233145982\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 11 | phase: train, loss=0.0013798419274210643\n",
      "logkey loss: 0.0, hyper loss: 0.013798419063767562\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 12 | phase: train, loss=0.001077057061849747\n",
      "logkey loss: 0.0, hyper loss: 0.010770570400218742\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 13 | phase: train, loss=0.0008514177517589325\n",
      "logkey loss: 0.0, hyper loss: 0.008514177384010205\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 14 | phase: train, loss=0.0006847892638269024\n",
      "logkey loss: 0.0, hyper loss: 0.006847892538644373\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 15 | phase: train, loss=0.0005689963470034015\n",
      "logkey loss: 0.0, hyper loss: 0.005689963364066222\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 16 | phase: train, loss=0.0004705910136973939\n",
      "logkey loss: 0.0, hyper loss: 0.004705910072423136\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 17 | phase: train, loss=0.00040531903356074903\n",
      "logkey loss: 0.0, hyper loss: 0.004053190287101107\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 18 | phase: train, loss=0.00033328159220101766\n",
      "logkey loss: 0.0, hyper loss: 0.0033328158755559856\n",
      "\n",
      "Log saved\n",
      "\n",
      "\n",
      "start calculate center\n",
      "Epoch: 19 | phase: train, loss=0.00028439162064713833\n",
      "logkey loss: 0.0, hyper loss: 0.0028439161534874868\n",
      "\n",
      "Log saved\n",
      " Model Saved on: /home/maslovapo_user/output/best_bert.pth\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "dirname = os.path.dirname('~')\n",
    "filename = os.path.join(dirname, '~/logbert')\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "from collections import defaultdict as DD\n",
    "\n",
    "# https://gist.github.com/KirillVladimirov/005ec7f762293d2321385580d3dbe335\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "options = dict()\n",
    "options['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "options[\"output_dir\"] = \"/home/maslovapo_user/output/\"\n",
    "options[\"model_dir\"] = options[\"output_dir\"] \n",
    "options[\"model_path\"] = options[\"model_dir\"] + \"best_bert.pth\"\n",
    "options[\"train_vocab\"] = options[\"output_dir\"] \n",
    "options[\"vocab_path\"] = options[\"output_dir\"] + \"vocab\"\n",
    "\n",
    "options[\"window_size\"] = 128\n",
    "options[\"adaptive_window\"] = True\n",
    "options[\"seq_len\"] = 512\n",
    "options[\"max_len\"] = 512 # for position embedding\n",
    "options[\"min_len\"] = 10\n",
    "options[\"mask_ratio\"] = 0.65\n",
    "# sample ratio\n",
    "options[\"train_ratio\"] = 1\n",
    "options[\"valid_ratio\"] = 0\n",
    "options[\"test_ratio\"] = 1\n",
    "\n",
    "# features\n",
    "options[\"is_logkey\"] = False\n",
    "options[\"is_time\"] = False\n",
    "\n",
    "options[\"hypersphere_loss\"] = True\n",
    "options[\"hypersphere_loss_test\"] = False\n",
    "\n",
    "options[\"scale\"] = None # MinMaxScaler()\n",
    "options[\"scale_path\"] = options[\"model_dir\"] + \"scale.pkl\"\n",
    "\n",
    "# model\n",
    "options[\"hidden\"] = 256 # embedding size\n",
    "options[\"layers\"] = 4\n",
    "options[\"attn_heads\"] = 4\n",
    "\n",
    "options[\"epochs\"] = 20\n",
    "options[\"n_epochs_stop\"] = 10\n",
    "options[\"batch_size\"] = 32\n",
    "\n",
    "options[\"corpus_lines\"] = None\n",
    "options[\"on_memory\"] = True\n",
    "options[\"num_workers\"] = 5\n",
    "options[\"lr\"] = 1e-3\n",
    "options[\"adam_beta1\"] = 0.9\n",
    "options[\"adam_beta2\"] = 0.999\n",
    "options[\"adam_weight_decay\"] = 0.00\n",
    "options[\"with_cuda\"]= True\n",
    "options[\"cuda_devices\"] = None\n",
    "options[\"log_freq\"] = None\n",
    "\n",
    "# predict\n",
    "options[\"num_candidates\"] = 6\n",
    "options[\"gaussian_mean\"] = 0\n",
    "options[\"gaussian_std\"] = 1\n",
    "\n",
    "seed_everything(seed=1234)\n",
    "\n",
    "if not os.path.exists(options['model_dir']):\n",
    "    os.makedirs(options['model_dir'], exist_ok=True)\n",
    "\n",
    "print(\"device\", options[\"device\"])\n",
    "print(\"features logkey:{} time: {}\\n\".format(options[\"is_logkey\"], options[\"is_time\"]))\n",
    "print(\"mask ratio\", options[\"mask_ratio\"]) \n",
    "\n",
    "Trainer(options).train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd5742-339b-46a4-bd8a-f5fb930424ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
