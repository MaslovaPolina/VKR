{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66e284a-f9da-48a2-bd23-c02b728de713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo-mirror.mos.ru/repository/pypi-proxy/simple\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.92)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (1.13.1+cu116)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2023.5.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.92 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.29.92)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.92->boto3) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.92->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.92->boto3) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 torch regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372e9318-08cb-4af3-a795-146c9bb3efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuditdDataloader:\n",
    "    def __init__(self, minio_config: dict, bucket: str, key: str, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "        s3 = boto3.resource(**minio_config)\n",
    "        dataobj = s3.Object(bucket_name=bucket, key=key)\n",
    "        buffer = io.BytesIO()\n",
    "        dataobj.download_fileobj(buffer)\n",
    "        self.parquet_file = pq.ParquetFile(buffer)\n",
    "        buffer.flush()\n",
    "        \n",
    "\n",
    "    def loader(self):\n",
    "        return self.parquet_file.iter_batches(batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457d79e8-d926-4c8e-8ded-c031e6692a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "import boto3\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "minio_config =  {'service_name': 's3',\n",
    "                  'endpoint_url': 'http://data-minio.udrvs-prod-aiplatform-tools-minio.svc.cluster.local:9000',\n",
    "                  'aws_access_key_id': '6q5iZHIIvA62s8tD',\n",
    "                  'aws_secret_access_key': 'maiHOzSqdsY5OOOTxvYVl3L2lgTXBVLO',\n",
    "                  'config': Config(signature_version='s3v4'),\n",
    "                  'region_name': 'us-east-1'}\n",
    "\n",
    "bucket = 'udrvs-prod-aiplatform-data-dud-ds'\n",
    "key = 'data/auditd/final_dataset.parquet'\n",
    "\n",
    "audit_data_loader = AuditdDataloader(minio_config=minio_config, bucket=bucket, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65e8993-86a6-4a6c-a5c7-cd2d9a855f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append(\"~/\")\n",
    "\n",
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1):\n",
    "        #print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "        for line in texts:\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = str(line).replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60537dd9-010a-449e-abc0-f772fbd3a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window(line, window_size, adaptive_window, seq_len=None, min_len=0):\n",
    "    \n",
    "\n",
    "    # filter the line/session shorter than 10\n",
    "    if len(line) < min_len:\n",
    "        return [], []\n",
    "\n",
    "    # max seq len\n",
    "    if seq_len is not None:\n",
    "        line = line[:seq_len]\n",
    "\n",
    "    if adaptive_window:\n",
    "        window_size = len(line)\n",
    "\n",
    "    line = np.array(line, dtype=object)\n",
    "    line = line.squeeze()\n",
    "        # if time duration doesn't exist, then create a zero array for time\n",
    "    tim = np.zeros(line.shape)\n",
    "\n",
    "    logkey_seqs = []\n",
    "    \n",
    "    for i in range(0, len(line), window_size):\n",
    "        logkey_seqs.append(line[i:i + window_size])\n",
    "        \n",
    "\n",
    "    return logkey_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001b6ec7-2460-496c-a6d1-703a105e378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, log_corpus, vocab, seq_len, corpus_lines=None, encoding=\"utf-8\", on_memory=True, predict_mode=False, mask_ratio=0.15):\n",
    "        \"\"\"\n",
    "\n",
    "        :param corpus: log sessions/line\n",
    "        :param vocab: log events collection including pad, ukn ...\n",
    "        :param seq_len: max sequence length\n",
    "        :param corpus_lines: number of log sessions\n",
    "        :param encoding:\n",
    "        :param on_memory:\n",
    "        :param predict_mode: if predict\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.on_memory = on_memory\n",
    "        self.encoding = encoding\n",
    "\n",
    "        self.predict_mode = predict_mode\n",
    "        self.log_corpus = log_corpus\n",
    "        self.corpus_lines = len(log_corpus)\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k = self.log_corpus[idx]\n",
    "\n",
    "        k_masked, k_label, = self.random_item(k)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        k = [self.vocab.sos_index] + k_masked\n",
    "        k_label = [self.vocab.pad_index] + k_label\n",
    "        # k_label = [self.vocab.sos_index] + k_label\n",
    "\n",
    "        \n",
    "        return k, k_label, \n",
    "\n",
    "    def random_item(self, k):\n",
    "        tokens = list(k)\n",
    "        output_label = []\n",
    "\n",
    "       \n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            \n",
    "            prob = random.random()\n",
    "            # replace 15% of tokens in a sequence to a masked token\n",
    "            if prob < self.mask_ratio:\n",
    "                # raise AttributeError(\"no mask in visualization\")\n",
    "\n",
    "                if self.predict_mode:\n",
    "                    tokens[i] = self.vocab.mask_index\n",
    "                    output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "                prob /= self.mask_ratio\n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = self.vocab.mask_index\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = random.randrange(len(self.vocab))\n",
    "\n",
    "                # 10% randomly change token to current token\n",
    "                else:\n",
    "                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "\n",
    "                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "\n",
    "                \n",
    "\n",
    "            else:\n",
    "                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "                output_label.append(0)\n",
    "                \n",
    "        return tokens, output_label, \n",
    "\n",
    "    def collate_fn(self, batch, percentile=100, dynamical_pad=True):\n",
    "        lens = [len(seq[0]) for seq in batch]\n",
    "\n",
    "        # find the max len in each batch\n",
    "        if dynamical_pad:\n",
    "            # dynamical padding\n",
    "            seq_len = int(np.percentile(lens, percentile))\n",
    "            if self.seq_len is not None:\n",
    "                seq_len = min(seq_len, self.seq_len)\n",
    "        else:\n",
    "            # fixed length padding\n",
    "            seq_len = self.seq_len\n",
    "\n",
    "        output =  defaultdict(list)\n",
    "        for seq in batch:\n",
    "            bert_input = seq[0][:seq_len]\n",
    "            bert_label = seq[1][:seq_len]\n",
    "            \n",
    "\n",
    "            padding = [self.vocab.pad_index for _ in range(seq_len - len(bert_input))]\n",
    "            bert_input.extend(padding), bert_label.extend(padding)\n",
    "\n",
    "           \n",
    "            output[\"bert_input\"].append(bert_input)\n",
    "            output[\"bert_label\"].append(bert_label)\n",
    "            #output[\"bert_input\"] = bert_input\n",
    "            #output[\"bert_label\"] = bert_label\n",
    "            \n",
    "        output[\"bert_input\"] = torch.tensor(output[\"bert_input\"], dtype=torch.long)\n",
    "        output[\"bert_label\"] = torch.tensor(output[\"bert_label\"], dtype=torch.long)\n",
    "        \n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04491dd0-d568-4690-80bb-8b2811c381c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from bert_pytorch.dataset import LogDataset\n",
    "\n",
    "\n",
    "def compute_anomaly(results, params, seq_threshold=0.5):\n",
    "    is_logkey = params[\"is_logkey\"]\n",
    "    \n",
    "    i = []\n",
    "    j = -1\n",
    "    for seq_res in results:\n",
    "        # label pairs as anomaly when over half of masked tokens are undetected\n",
    "        j+=1\n",
    "        if (is_logkey and seq_res[\"undetected_tokens\"] > seq_res[\"masked_tokens\"] * seq_threshold) or \\\n",
    "                (params[\"hypersphere_loss_test\"] and seq_res[\"deepSVDD_label\"]):\n",
    "            \n",
    "            i.append(j)\n",
    "    return i\n",
    "\n",
    "\n",
    "class Predictor():\n",
    "    def __init__(self, options):\n",
    "        self.model_path = options[\"model_path\"]\n",
    "        self.vocab_path = options[\"vocab_path\"]\n",
    "        self.device = options[\"device\"]\n",
    "        self.window_size = options[\"window_size\"]\n",
    "        self.adaptive_window = options[\"adaptive_window\"]\n",
    "        self.seq_len = options[\"seq_len\"]\n",
    "        self.corpus_lines = options[\"corpus_lines\"]\n",
    "        self.on_memory = options[\"on_memory\"]\n",
    "        self.batch_size = options[\"batch_size\"]\n",
    "        self.num_workers = options[\"num_workers\"]\n",
    "        self.num_candidates = options[\"num_candidates\"]\n",
    "        self.output_dir = options[\"output_dir\"]\n",
    "        self.model_dir = options[\"model_dir\"]\n",
    "        self.gaussian_mean = options[\"gaussian_mean\"]\n",
    "        self.gaussian_std = options[\"gaussian_std\"]\n",
    "\n",
    "        self.is_logkey = options[\"is_logkey\"]\n",
    "        self.is_time = options[\"is_time\"]\n",
    "        self.scale_path = options[\"scale_path\"]\n",
    "\n",
    "        self.hypersphere_loss = options[\"hypersphere_loss\"]\n",
    "        self.hypersphere_loss_test = options[\"hypersphere_loss_test\"]\n",
    "\n",
    "        self.lower_bound = self.gaussian_mean - 3 * self.gaussian_std\n",
    "        self.upper_bound = self.gaussian_mean + 3 * self.gaussian_std\n",
    "\n",
    "        self.center = None\n",
    "        self.radius = None\n",
    "        self.test_ratio = options[\"test_ratio\"]\n",
    "        self.mask_ratio = options[\"mask_ratio\"]\n",
    "        self.min_len=options[\"min_len\"]\n",
    "\n",
    "    def detect_logkey_anomaly(self, masked_output, masked_label):\n",
    "        num_undetected_tokens = 0\n",
    "        output_maskes = []\n",
    "        for i, token in enumerate(masked_label):\n",
    "            # output_maskes.append(torch.argsort(-masked_output[i])[:30].cpu().numpy()) # extract top 30 candidates for mask labels\n",
    "\n",
    "            if token not in torch.argsort(-masked_output[i])[:self.num_candidates]:\n",
    "                num_undetected_tokens += 1\n",
    "\n",
    "        return num_undetected_tokens, [output_maskes, masked_label.cpu().numpy()]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_test(seq, window_size, adaptive_window, seq_len, scale, min_len):\n",
    "        \"\"\"\n",
    "        :return: log_seqs: num_samples x session(seq)_length, tim_seqs: num_samples x session_length\n",
    "        \"\"\"\n",
    "        log_seqs = []\n",
    "        \n",
    "        for line in seq:\n",
    "                #if idx > 40: break\n",
    "                log_seq = fixed_window(line, window_size, adaptive_window=adaptive_window,seq_len=seq_len, min_len=min_len)\n",
    "                if len(log_seq) == 0:\n",
    "                    continue\n",
    "\n",
    "                # if scale is not None:\n",
    "                #     times = tim_seq\n",
    "                #     for i, tn in enumerate(times):\n",
    "                #         tn = np.array(tn).reshape(-1, 1)\n",
    "                #         times[i] = scale.transform(tn).reshape(-1).tolist()\n",
    "                #     tim_seq = times\n",
    "\n",
    "                log_seqs += log_seq\n",
    "                \n",
    "\n",
    "        # sort seq_pairs by seq len\n",
    "        log_seqs = np.array(log_seqs)\n",
    "        \n",
    "\n",
    "        test_len = list(map(len, log_seqs))\n",
    "        test_sort_index = np.argsort(-1 * np.array(test_len))\n",
    "\n",
    "        log_seqs = log_seqs[test_sort_index]\n",
    "        \n",
    "\n",
    "        print(f\"test size: {len(log_seqs)}\")\n",
    "        return log_seqs\n",
    "\n",
    "    def helper(self, model, vocab, scale=None, error_dict=None):\n",
    "        total_results = []\n",
    "        total_errors = []\n",
    "        output_results = []\n",
    "        total_dist = []\n",
    "        output_cls = []\n",
    "        logkey_test = self.generate_test(X_test, self.window_size, self.adaptive_window, self.seq_len, scale, self.min_len)\n",
    "\n",
    "        # use 1/10 test data\n",
    "        if self.test_ratio != 1:\n",
    "            num_test = len(logkey_test)\n",
    "            rand_index = torch.randperm(num_test)\n",
    "            rand_index = rand_index[:int(num_test * self.test_ratio)] if isinstance(self.test_ratio, float) else rand_index[:self.test_ratio]\n",
    "            logkey_test = logkey_test[rand_index]\n",
    "\n",
    "\n",
    "        seq_dataset = LogDataset(logkey_test, vocab, seq_len=self.seq_len,\n",
    "                                 corpus_lines=self.corpus_lines, on_memory=self.on_memory, predict_mode=True, mask_ratio=self.mask_ratio)\n",
    "\n",
    "        # use large batch size in test data\n",
    "        data_loader = DataLoader(seq_dataset, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                                 collate_fn=seq_dataset.collate_fn)\n",
    "\n",
    "        for idx, data in enumerate(data_loader):\n",
    "           \n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            result = model(data[\"bert_input\"])\n",
    "\n",
    "            # mask_lm_output, mask_tm_output: batch_size x session_size x vocab_size\n",
    "            # cls_output: batch_size x hidden_size\n",
    "            # bert_label, time_label: batch_size x session_size\n",
    "            # in session, some logkeys are masked\n",
    "\n",
    "            mask_lm_output = result[\"logkey_output\"]\n",
    "            output_cls += result[\"cls_output\"].tolist()\n",
    "\n",
    "            # dist = torch.sum((result[\"cls_output\"] - self.hyper_center) ** 2, dim=1)\n",
    "            # when visualization no mask\n",
    "            # continue\n",
    "\n",
    "            # loop though each session in batch\n",
    "            for i in range(len(data[\"bert_label\"])):\n",
    "                seq_results = {\"num_error\": 0,\n",
    "                               \"undetected_tokens\": 0,\n",
    "                               \"masked_tokens\": 0,\n",
    "                               \"total_logkey\": torch.sum(data[\"bert_input\"][i] > 0).item(),\n",
    "                               \"deepSVDD_label\": 0\n",
    "                               }\n",
    "\n",
    "                mask_index = data[\"bert_label\"][i] > 0\n",
    "                num_masked = torch.sum(mask_index).tolist()\n",
    "                seq_results[\"masked_tokens\"] = num_masked\n",
    "\n",
    "                if self.is_logkey:\n",
    "                    num_undetected, output_seq = self.detect_logkey_anomaly(\n",
    "                        mask_lm_output[i][mask_index], data[\"bert_label\"][i][mask_index])\n",
    "                    seq_results[\"undetected_tokens\"] = num_undetected\n",
    "\n",
    "                    output_results.append(output_seq)\n",
    "\n",
    "                if self.hypersphere_loss_test:\n",
    "                    # detect by deepSVDD distance\n",
    "                    assert result[\"cls_output\"][i].size() == self.center.size()\n",
    "                    # dist = torch.sum((result[\"cls_fnn_output\"][i] - self.center) ** 2)\n",
    "                    dist = torch.sqrt(torch.sum((result[\"cls_output\"][i] - self.center) ** 2))\n",
    "                    total_dist.append(dist.item())\n",
    "\n",
    "                    # user defined threshold for deepSVDD_label\n",
    "                    seq_results[\"deepSVDD_label\"] = int(dist.item() > self.radius)\n",
    "                    #\n",
    "                    # if dist > 0.25:\n",
    "                    #     pass\n",
    "\n",
    "                \n",
    "                total_results.append(seq_results)\n",
    "\n",
    "        # for time\n",
    "        # return total_results, total_errors\n",
    "\n",
    "        #for logkey\n",
    "        #return total_results, output_results\n",
    "\n",
    "        # for hypersphere distance\n",
    "        \n",
    "        return total_results, output_cls\n",
    "\n",
    "    def predict(self):\n",
    "        model = torch.load(self.model_path)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        print('model_path: {}'.format(self.model_path))\n",
    "\n",
    "        start_time = time.time()\n",
    "        vocab = WordVocab.load_vocab(self.vocab_path)\n",
    "\n",
    "        scale = None\n",
    "        error_dict = None\n",
    "        \n",
    "        if self.hypersphere_loss:\n",
    "            center_dict = torch.load(self.model_dir + \"best_center.pt\")\n",
    "            self.center = center_dict[\"center\"]\n",
    "            self.radius = center_dict[\"radius\"]\n",
    "            # self.center = self.center.view(1,-1)\n",
    "\n",
    "        scale=None\n",
    "        error_dict=None\n",
    "        print(\"Test normal predicting\")\n",
    "        test_normal_results, test_normal_errors = self.helper(model, vocab, scale, error_dict)\n",
    "        params = {\"is_logkey\": self.is_logkey, \"is_time\": self.is_time, \"hypersphere_loss\": self.hypersphere_loss,\n",
    "                  \"hypersphere_loss_test\": self.hypersphere_loss_test}\n",
    "        \n",
    "        res = compute_anomaly(test_normal_results, params, seq_threshold=0.5)\n",
    "        print(\"Saving test  results\")\n",
    "        with open(self.model_dir + \"test_results.csv\", \"w\") as f:\n",
    "            write = csv.writer(f)\n",
    "            write.writerows(res)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "416ed4ee-14a8-4a46-8467-1bb12fb278ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "X_test = []\n",
    "\n",
    "for i, X in enumerate(audit_data_loader.loader()):\n",
    "    X_test.append(X.to_pandas().auditd.values)\n",
    "    if i == 20000:\n",
    "        break\n",
    "X_test = X_test[10001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6312a5-025c-4dd2-b75a-7eda11bdf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, max_len=512, hidden=768, n_layers=12, attn_heads=12, dropout=0.1, is_logkey=True, is_time=False):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = hidden * 2\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden, max_len=max_len, is_logkey=is_logkey, is_time=is_time)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 2, dropout) for _ in range(n_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, x, segment_info=None, time_info=None):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "       \n",
    "        mask = (x>0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info, time_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67329592-6933-49cb-a361-573cfd7c1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BERTLog(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mask_lm = MaskedLogModel(self.bert.hidden, vocab_size)\n",
    "        # self.fnn_cls = LinearCLS(self.bert.hidden)\n",
    "        #self.cls_lm = LogClassifier(self.bert.hidden)\n",
    "        self.result = {\"logkey_output\": None, \"cls_output\": None, \"cls_fnn_output\": None}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "\n",
    "        self.result[\"logkey_output\"] = self.mask_lm(x)\n",
    "       \n",
    "        # self.result[\"cls_output\"] = x.float().mean(axis=1) #x[:, 0]\n",
    "        self.result[\"cls_output\"] = x[:, 0]\n",
    "        # self.result[\"cls_output\"] = self.fnn_cls(x[:, 0])\n",
    "\n",
    "        # print(self.result[\"cls_fnn_output\"].shape)\n",
    "\n",
    "        return self.result\n",
    "\n",
    "class MaskedLogModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "\n",
    "class TimeLogModel(nn.Module):\n",
    "    def __init__(self, hidden, time_size=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, time_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class LogClassifier(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, hidden)\n",
    "\n",
    "    def forward(self, cls):\n",
    "        return self.linear(cls)\n",
    "\n",
    "class LinearCLS(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "befb3fe9-eab1-4a97-af7b-daad65714ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b46cff-79f1-41bf-8a86-b09e21d5641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e94a0da-320a-482b-a4c7-2e2747537154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0)\n",
    "        \n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Linear(1, embed_size)\n",
    "\n",
    "    def forward(self, time_interval):\n",
    "        return self.time_embed(time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b747ff-8880-48fa-aa8f-e7e380c88827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1, is_logkey=True, is_time=False):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim, max_len=max_len)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.time_embed = TimeEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "        self.is_logkey = is_logkey\n",
    "        self.is_time = is_time\n",
    "\n",
    "    def forward(self, sequence, segment_label=None, time_info=None):\n",
    "        x = self.position(sequence)\n",
    "        # if self.is_logkey:\n",
    "        x = x + self.token(sequence)\n",
    "        if segment_label is not None:\n",
    "            x = x + self.segment(segment_label)\n",
    "        if self.is_time:\n",
    "            x = x + self.time_embed(time_info)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c7622d7-f708-42fc-9a56-3b0596e643b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39f9341-c488-449a-a606-7cbc0de6fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbbd792a-3776-4ea3-ade4-273bdfa6f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "   \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caa34518-ed2f-40cc-a8b3-d32367036f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "   \n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "600ad7e7-e47a-4050-b89f-8ef809dc2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "   \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03c0b945-862a-40ef-9e41-9bdb8c9db332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "795f043e-e00d-4f23-99f9-19d292656a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeb04326-6011-4a6d-9ba6-a4496179a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "features logkey:True time: False\n",
      "\n",
      "mask ratio 0.65\n",
      "model_path: /home/maslovapo_user/output/best_bert.pth\n",
      "Test normal predicting\n",
      "test size: 10000\n",
      "Saving test  results\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "options = dict()\n",
    "options['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "options[\"output_dir\"] = \"/home/maslovapo_user/output/\"\n",
    "options[\"model_dir\"] = options[\"output_dir\"] \n",
    "options[\"model_path\"] = options[\"model_dir\"] + \"best_bert.pth\"\n",
    "options[\"train_vocab\"] = options[\"output_dir\"] \n",
    "options[\"vocab_path\"] = options[\"output_dir\"] + \"vocab\"\n",
    "\n",
    "options[\"window_size\"] = 128\n",
    "options[\"adaptive_window\"] = True\n",
    "options[\"seq_len\"] = 512\n",
    "options[\"max_len\"] = 512 # for position embedding\n",
    "options[\"min_len\"] = 10\n",
    "options[\"mask_ratio\"] = 0.65\n",
    "# sample ratio\n",
    "options[\"train_ratio\"] = 1\n",
    "options[\"valid_ratio\"] = 0\n",
    "options[\"test_ratio\"] = 1\n",
    "\n",
    "# features\n",
    "options[\"is_logkey\"] = True\n",
    "options[\"is_time\"] = False\n",
    "\n",
    "options[\"hypersphere_loss\"] = True\n",
    "options[\"hypersphere_loss_test\"] = False\n",
    "\n",
    "options[\"scale\"] = None # MinMaxScaler()\n",
    "options[\"scale_path\"] = options[\"model_dir\"] + \"scale.pkl\"\n",
    "\n",
    "# model\n",
    "options[\"hidden\"] = 256 # embedding size\n",
    "options[\"layers\"] = 4\n",
    "options[\"attn_heads\"] = 4\n",
    "\n",
    "options[\"epochs\"] = 20\n",
    "options[\"n_epochs_stop\"] = 10\n",
    "options[\"batch_size\"] = 32\n",
    "\n",
    "options[\"corpus_lines\"] = None\n",
    "options[\"on_memory\"] = True\n",
    "options[\"num_workers\"] = 5\n",
    "options[\"lr\"] = 1e-3\n",
    "options[\"adam_beta1\"] = 0.9\n",
    "options[\"adam_beta2\"] = 0.999\n",
    "options[\"adam_weight_decay\"] = 0.00\n",
    "options[\"with_cuda\"]= True\n",
    "options[\"cuda_devices\"] = None\n",
    "options[\"log_freq\"] = None\n",
    "\n",
    "# predict\n",
    "options[\"num_candidates\"] = 6\n",
    "options[\"gaussian_mean\"] = 0\n",
    "options[\"gaussian_std\"] = 1\n",
    "\n",
    "print(\"device\", options[\"device\"])\n",
    "print(\"features logkey:{} time: {}\\n\".format(options[\"is_logkey\"], options[\"is_time\"]))\n",
    "print(\"mask ratio\", options[\"mask_ratio\"])\n",
    "\n",
    "Predictor(options).predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e15dd7-3cc7-4e92-8320-04150c6955e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
