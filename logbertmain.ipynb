{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5790535-a4c1-4f1b-9957-55b2a229c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo-mirror.mos.ru/repository/pypi-proxy/simple\n",
      "Collecting torch\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/torch/2.0.1/torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "     |████████████████████████████████| 619.9 MB 25 kB/s                             | 5.8 MB 16.2 MB/s eta 0:00:38                             | 9.1 MB 16.2 MB/s eta 0:00:38MB/s eta 0:00:38MB/s eta 0:00:38    |█▏                              | 23.4 MB 16.2 MB/s eta 0:00:37��▍                              | 26.8 MB 16.2 MB/s eta 0:00:37    |█▋                              | 30.7 MB 16.2 MB/s eta 0:00:37    |██                              | 37.4 MB 16.2 MB/s eta 0:00:37    |██                              | 40.5 MB 16.2 MB/s eta 0:00:36                     | 43.8 MB 16.2 MB/s eta 0:00:36                     | 46.5 MB 16.2 MB/s eta 0:00:36                         | 49.4 MB 16.2 MB/s eta 0:00:36                     | 52.1 MB 45.7 MB/s eta 0:00:130:13                     | 57.1 MB 45.7 MB/s eta 0:00:13                     | 59.9 MB 45.7 MB/s eta 0:00:13:00:139.3 MB 45.7 MB/s eta 0:00:13▊                            | 71.3 MB 45.7 MB/s eta 0:00:134.6 MB 45.7 MB/s eta 0:00:12███                            | 76.4 MB 45.7 MB/s eta 0:00:129.5 MB 45.7 MB/s eta 0:00:12██▏                          | 101.0 MB 45.7 MB/s eta 0:00:12MB/s eta 0:00:05MB/s eta 0:00:05 113.6 MB 106.6 MB/s eta 0:00:05MB/s eta 0:00:05█▎                         | 121.5 MB 106.6 MB/s eta 0:00:05█▍                         | 124.3 MB 106.6 MB/s eta 0:00:05█▋                         | 127.0 MB 106.6 MB/s eta 0:00:05█▊                         | 129.8 MB 106.6 MB/s eta 0:00:05         | 132.6 MB 106.6 MB/s eta 0:00:05B 106.6 MB/s eta 0:00:05 0:00:05B 106.6 MB/s eta 0:00:05B 106.6 MB/s eta 0:00:05B 47.8 MB/s eta 0:00:10 B 47.8 MB/s eta 0:00:10 68.4 MB 47.8 MB/s eta 0:00:10 ��███▉                       | 171.6 MB 47.8 MB/s eta 0:00:10 a 0:00:10 B 47.8 MB/s eta 0:00:10                | 187.5 MB 47.8 MB/s eta 0:00:10 :09 B 47.8 MB/s eta 0:00:09          | 196.5 MB 47.8 MB/s eta 0:00:09 ��███▎                     | 199.0 MB 47.8 MB/s eta 0:00:09 ��███▊                     | 207.6 MB 34.3 MB/s eta 0:00:13 ��███▉                     | 210.9 MB 34.3 MB/s eta 0:00:12 ��████                     | 213.3 MB 34.3 MB/s eta 0:00:12 B/s eta 0:00:12 B/s eta 0:00:12 B/s eta 0:00:12 B/s eta 0:00:12 █▏                   | 234.8 MB 34.3 MB/s eta 0:00:12 █▍                   | 240.3 MB 34.3 MB/s eta 0:00:12  MB/s eta 0:00:11 █▋                   | 245.4 MB 34.3 MB/s eta 0:00:11    | 248.1 MB 34.3 MB/s eta 0:00:11 ██                   | 250.7 MB 34.3 MB/s eta 0:00:11    | 253.9 MB 34.3 MB/s eta 0:00:11        | 281.0 MB 86.7 MB/s eta 0:00:04        | 284.9 MB 86.7 MB/s eta 0:00:04        | 290.3 MB 86.7 MB/s eta 0:00:04 �█████████▍                | 298.6 MB 86.7 MB/s eta 0:00:04 �█████████▌                | 301.3 MB 13.0 MB/s eta 0:00:25 �█████████▊                | 304.0 MB 13.0 MB/s eta 0:00:25 �█████████▉                | 307.0 MB 13.0 MB/s eta 0:00:25 �██████████                | 310.0 MB 13.0 MB/s eta 0:00:24 s eta 0:00:24 s eta 0:00:24 s eta 0:00:24 .0 MB 13.0 MB/s eta 0:00:23 s eta 0:00:23 s eta 0:00:23 s eta 0:00:23 ��▍              | 337.4 MB 13.0 MB/s eta 0:00:22 ��▌              | 340.0 MB 13.0 MB/s eta 0:00:22 ��▊              | 343.0 MB 13.0 MB/s eta 0:00:22 ��█              | 350.7 MB 13.0 MB/s eta 0:00:21 ███████▎             | 353.7 MB 85.5 MB/s eta 0:00:04 ��█████████████▍             | 356.5 MB 85.5 MB/s eta 0:00:04 �▌             | 358.6 MB 85.5 MB/s eta 0:00:04 ��█████████████▋             | 361.5 MB 85.5 MB/s eta 0:00:04 ��█████████████▉             | 364.8 MB 85.5 MB/s eta 0:00:03 ��██████████████             | 367.9 MB 85.5 MB/s eta 0:00:03 ��████             | 369.9 MB 85.5 MB/s eta 0:00:03 B/s eta 0:00:03 B/s eta 0:00:03 B/s eta 0:00:03 ��██████████▋            | 380.3 MB 85.5 MB/s eta 0:00:03 B/s eta 0:00:03 ��█▋           | 400.3 MB 45.0 MB/s eta 0:00:05 ��██           | 407.9 MB 45.0 MB/s eta 0:00:05 ██████████████▏          | 410.6 MB 45.0 MB/s eta 0:00:05 ██████████████▎          | 413.2 MB 45.0 MB/s eta 0:00:05 ��████▌          | 416.0 MB 45.0 MB/s eta 0:00:05 ██████████████▊          | 419.8 MB 45.0 MB/s eta 0:00:05 ███████████████          | 424.7 MB 45.0 MB/s eta 0:00:05 ███████████████          | 427.5 MB 45.0 MB/s eta 0:00:05 00:05 00:05 00:05  MB/s eta 0:00:04 00:04 00:04        | 449.7 MB 102.6 MB/s eta 0:00:02████████████▎        | 451.8 MB 102.6 MB/s eta 0:00:02       | 455.3 MB 102.6 MB/s eta 0:00:02████████████▊        | 460.5 MB 102.6 MB/s eta 0:00:02�██████████▎       | 470.8 MB 102.6 MB/s eta 0:00:02�██████████▍       | 473.3 MB 102.6 MB/s eta 0:00:02�██████████▊       | 478.1 MB 102.6 MB/s eta 0:00:02�███████████       | 482.5 MB 102.6 MB/s eta 0:00:02�███████████       | 485.5 MB 156 kB/s eta 0:14:22  ██████████████████████▍      | 492.6 MB 156 kB/s eta 0:13:37  ██████████████████████▊      | 497.6 MB 156 kB/s eta 0:13:05  ██████████████████████▉      | 500.7 MB 156 kB/s eta 0:12:44  ███████████████████████      | 503.6 MB 156 kB/s eta 0:12:26  ��█████████████████      | 505.6 MB 156 kB/s eta 0:12:13   508.8 MB 156 kB/s eta 0:11:53  /s eta 0:11:32  /s eta 0:11:16   517.3 MB 156 kB/s eta 0:10:58  /s eta 0:10:39  ��████████     | 522.8 MB 156 kB/s eta 0:10:23  ��██▏    | 525.5 MB 156 kB/s eta 0:10:06  ��██▍    | 530.9 MB 156 kB/s eta 0:09:31  ��██▋    | 534.0 MB 12.4 MB/s eta 0:00:07 ��██▉    | 539.2 MB 12.4 MB/s eta 0:00:07 ��███    | 542.5 MB 12.4 MB/s eta 0:00:07 ��█████████████▏   | 545.3 MB 12.4 MB/s eta 0:00:07 �██████████▎   | 548.0 MB 12.4 MB/s eta 0:00:06 ��███████████████████████▍   | 550.1 MB 12.4 MB/s eta 0:00:06 ��█████████████▋   | 555.2 MB 12.4 MB/s eta 0:00:06 ��████████████████████████▏  | 565.3 MB 12.4 MB/s eta 0:00:05 ��████████████████████████▍  | 568.4 MB 12.4 MB/s eta 0:00:05 ��████████████████████████▌  | 571.2 MB 12.4 MB/s eta 0:00:04 ��████████████████████████▋  | 573.9 MB 12.4 MB/s eta 0:00:04 ███████████████████  | 579.2 MB 12.4 MB/s eta 0:00:04 �████████████  | 582.1 MB 12.9 MB/s eta 0:00:03 a 0:00:03 a 0:00:03 .9 MB/s eta 0:00:02 a 0:00:02 a 0:00:02 �█▏| 603.1 MB 12.9 MB/s eta 0:00:02 �█▌| 610.9 MB 12.9 MB/s eta 0:00:01 �█▊| 613.7 MB 12.9 MB/s eta 0:00:01 �█▉| 617.1 MB 12.9 MB/s eta 0:00:01 \n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/triton/2.0.0/triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "     |████████████████████████████████| 63.3 MB 47.8 MB/s            ta 0:00:05MB/s eta 0:00:04         | 30.9 MB 10.5 MB/s eta 0:00:04�██▊               | 33.1 MB 10.5 MB/s eta 0:00:03��█▍             | 36.3 MB 10.5 MB/s eta 0:00:03��██████████████▋            | 38.8 MB 10.5 MB/s eta 0:00:03MB/s eta 0:00:03��███          | 43.5 MB 47.8 MB/s eta 0:00:01████████████████         | 45.5 MB 47.8 MB/s eta 0:00:01��       | 48.1 MB 47.8 MB/s eta 0:00:01�███████████▋      | 50.7 MB 47.8 MB/s eta 0:00:01 0:00:01��███▎   | 56.1 MB 47.8 MB/s eta 0:00:01��██████████████▉  | 59.0 MB 47.8 MB/s eta 0:00:01ta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cudnn-cu11/8.5.0.96/nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "     |████████████████████████████████| 557.1 MB 18 kB/s               .1 MB 5.0 MB/s eta 0:01:51 | 7.9 MB 5.0 MB/s eta 0:01:51                             | 13.9 MB 5.0 MB/s eta 0:01:50              | 19.9 MB 5.0 MB/s eta 0:01:48              | 22.9 MB 5.0 MB/s eta 0:01:48              | 26.3 MB 5.0 MB/s eta 0:01:47                             | 29.3 MB 5.0 MB/s eta 0:01:46              | 32.2 MB 5.0 MB/s eta 0:01:46              | 35.0 MB 5.0 MB/s eta 0:01:458 MB 5.0 MB/s eta 0:01:454 MB 5.0 MB/s eta 0:01:448 MB 5.0 MB/s eta 0:01:431 MB 5.0 MB/s eta 0:01:435 MB 7.8 MB/s eta 0:01:05051:050404:01:0303                        | 76.2 MB 7.8 MB/s eta 0:01:02 84.4 MB 7.8 MB/s eta 0:01:011 MB 7.8 MB/s eta 0:01:007 MB 7.8 MB/s eta 0:01:00��████▊                          | 100.4 MB 71.6 MB/s eta 0:00:07��█████                          | 103.1 MB 71.6 MB/s eta 0:00:07        | 114.8 MB 71.6 MB/s eta 0:00:07        | 117.8 MB 71.6 MB/s eta 0:00:077666               | 144.8 MB 71.6 MB/s eta 0:00:06                    | 147.9 MB 71.6 MB/s eta 0:00:06               | 151.7 MB 71.6 MB/s eta 0:00:06               | 154.6 MB 53.7 MB/s eta 0:00:08�██                       | 158.5 MB 53.7 MB/s eta 0:00:080808080808              | 178.9 MB 53.7 MB/s eta 0:00:08              | 182.3 MB 53.7 MB/s eta 0:00:07              | 186.1 MB 53.7 MB/s eta 0:00:07             | 188.8 MB 53.7 MB/s eta 0:00:07              | 192.2 MB 53.7 MB/s eta 0:00:07 |███████████▎                    | 196.0 MB 53.7 MB/s eta 0:00:07��██████▋                    | 202.1 MB 53.7 MB/s eta 0:00:07 |███████████▉                    | 205.6 MB 53.7 MB/s eta 0:00:07 |████████████                    | 208.5 MB 53.7 MB/s eta 0:00:07                 | 211.7 MB 46.9 MB/s eta 0:00:08     | 214.4 MB 46.9 MB/s eta 0:00:08     | 218.2 MB 46.9 MB/s eta 0:00:08�████████▊                   | 220.8 MB 46.9 MB/s eta 0:00:08/s eta 0:00:08��███████                   | 227.2 MB 46.9 MB/s eta 0:00:08��████████▎                  | 230.8 MB 46.9 MB/s eta 0:00:07��████████▌                  | 234.5 MB 46.9 MB/s eta 0:00:07��████████▊                  | 238.9 MB 46.9 MB/s eta 0:00:07��█████████                  | 241.9 MB 46.9 MB/s eta 0:00:07�████████████▏                 | 245.7 MB 46.9 MB/s eta 0:00:076.9 MB/s eta 0:00:076.9 MB/s eta 0:00:076.9 MB/s eta 0:00:076.9 MB/s eta 0:00:07        | 261.0 MB 46.9 MB/s eta 0:00:07███▏                | 264.4 MB 46.9 MB/s eta 0:00:07███▌                | 269.4 MB 10.9 MB/s eta 0:00:27███▋                | 271.9 MB 10.9 MB/s eta 0:00:27███▉                | 275.3 MB 10.9 MB/s eta 0:00:26████                | 277.4 MB 10.9 MB/s eta 0:00:26████                | 280.0 MB 10.9 MB/s eta 0:00:26 |████████████████▏               | 282.4 MB 10.9 MB/s eta 0:00:26��███████████▍               | 285.1 MB 10.9 MB/s eta 0:00:26███████████████▌               | 287.9 MB 10.9 MB/s eta 0:00:25 |████████████████▉               | 293.6 MB 10.9 MB/s eta 0:00:25 |█████████████████               | 297.0 MB 10.9 MB/s eta 0:00:24 | 299.8 MB 10.9 MB/s eta 0:00:24��█████▌              | 304.7 MB 10.9 MB/s eta 0:00:24 | 307.7 MB 10.9 MB/s eta 0:00:23��█████▉              | 310.5 MB 10.9 MB/s eta 0:00:23 | 313.5 MB 10.9 MB/s eta 0:00:23��███████▏             | 316.4 MB 10.9 MB/s eta 0:00:23��███████▎             | 319.2 MB 5.6 MB/s eta 0:00:43 ��███████▌             | 322.3 MB 5.6 MB/s eta 0:00:42 ��███████▋             | 325.0 MB 5.6 MB/s eta 0:00:42  | 328.3 MB 5.6 MB/s eta 0:00:41     |███████████████████▏            | 334.5 MB 5.6 MB/s eta 0:00:40     |███████████████████▍            | 337.6 MB 5.6 MB/s eta 0:00:40     |███████████████████▌            | 340.2 MB 5.6 MB/s eta 0:00:39     |███████████████████▊            | 342.7 MB 5.6 MB/s eta 0:00:39     |███████████████████▉            | 345.8 MB 5.6 MB/s eta 0:00:38     |████████████████████            | 348.4 MB 5.6 MB/s eta 0:00:38   | 351.0 MB 5.6 MB/s eta 0:00:37   | 354.2 MB 5.6 MB/s eta 0:00:37   | 357.5 MB 5.6 MB/s eta 0:00:36   | 360.6 MB 5.6 MB/s eta 0:00:36   | 363.5 MB 5.6 MB/s eta 0:00:35 �████████           | 366.0 MB 5.6 MB/s eta 0:00:35 �████████▍          | 371.8 MB 7.1 MB/s eta 0:00:26 �████████▌          | 375.0 MB 7.1 MB/s eta 0:00:26 �████████▊          | 377.7 MB 7.1 MB/s eta 0:00:26 �█████████          | 383.1 MB 7.1 MB/s eta 0:00:25 ��████████████████████▏         | 386.2 MB 7.1 MB/s eta 0:00:24 �█████████████▍         | 389.7 MB 7.1 MB/s eta 0:00:24 ��████████████████████▊         | 395.3 MB 7.1 MB/s eta 0:00:23 ��████████████████████▉         | 398.3 MB 7.1 MB/s eta 0:00:23 �██████████████         | 401.9 MB 7.1 MB/s eta 0:00:22 ��████████▏        | 404.2 MB 7.1 MB/s eta 0:00:22 B 7.1 MB/s eta 0:00:21 B 7.1 MB/s eta 0:00:21 B 7.1 MB/s eta 0:00:21 B 7.1 MB/s eta 0:00:20 ██        | 419.7 MB 7.1 MB/s eta 0:00:20 █████▎       | 422.3 MB 6.0 MB/s eta 0:00:23 █████▍       | 424.4 MB 6.0 MB/s eta 0:00:23 █████▌       | 426.6 MB 6.0 MB/s eta 0:00:22 █████▋       | 429.3 MB 6.0 MB/s eta 0:00:22 █████▉       | 431.8 MB 6.0 MB/s eta 0:00:21 ██████       | 433.8 MB 6.0 MB/s eta 0:00:21 ��███       | 435.5 MB 6.0 MB/s eta 0:00:21 ��████████████████▏      | 438.1 MB 6.0 MB/s eta 0:00:20 ��████████████████▎      | 440.0 MB 6.0 MB/s eta 0:00:20 ��████████████████▍      | 442.5 MB 6.0 MB/s eta 0:00:20 .9 MB 6.0 MB/s eta 0:00:19 ███████████████▉      | 449.3 MB 6.0 MB/s eta 0:00:19 ��█████████████████      | 451.6 MB 6.0 MB/s eta 0:00:18 .8 MB 6.0 MB/s eta 0:00:18 �████████▉    | 484.2 MB 85.5 MB/s eta 0:00:01�█████████    | 486.4 MB 85.5 MB/s eta 0:00:01�█▏   | 489.5 MB 85.5 MB/s eta 0:00:01��████████▎   | 492.6 MB 85.5 MB/s eta 0:00:01��████████▌   | 495.9 MB 85.5 MB/s eta 0:00:01��████████▋   | 498.3 MB 85.5 MB/s eta 0:00:01��████████▊   | 500.7 MB 85.5 MB/s eta 0:00:01��████████▉   | 503.0 MB 85.5 MB/s eta 0:00:01��█████████   | 506.4 MB 85.5 MB/s eta 0:00:01��████████████████▎  | 508.9 MB 85.5 MB/s eta 0:00:01��████████████████████████▍  | 511.6 MB 101.1 MB/s eta 0:00:01�███████████▋  | 514.9 MB 101.1 MB/s eta 0:00:01�███████████▊  | 517.3 MB 101.1 MB/s eta 0:00:01ta 0:00:01ta 0:00:01ta 0:00:01�█▏| 543.3 MB 101.1 MB/s eta 0:00:01�█▍| 545.6 MB 101.1 MB/s eta 0:00:01��██████████████▌| 548.1 MB 101.1 MB/s eta 0:00:01��██████████████▋| 550.5 MB 101.1 MB/s eta 0:00:01�█▊| 553.1 MB 87.4 MB/s eta 0:00:01 �█▉| 555.1 MB 87.4 MB/s eta 0:00:01 \n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-nvtx-cu11/11.7.91/nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 5.9 MB/s             \n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cuda-nvrtc-cu11/11.7.99/nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "     |████████████████████████████████| 21.0 MB 12.6 MB/s            ████████████████         | 15.2 MB 12.6 MB/s eta 0:00:0101\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cuda-runtime-cu11/11.7.99/nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "     |████████████████████████████████| 849 kB 18.3 MB/s            \n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-curand-cu11/10.2.10.91/nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "     |████████████████████████████████| 54.6 MB 7.1 MB/s                                  | 5.7 MB 9.3 MB/s eta 0:00:06     | 8.8 MB 9.3 MB/s eta 0:00:05████                         | 11.8 MB 9.3 MB/s eta 0:00:055�▏                     | 17.3 MB 9.3 MB/s eta 0:00:05                | 20.2 MB 9.3 MB/s eta 0:00:04 |████████████▊                   | 21.7 MB 9.3 MB/s eta 0:00:04��█████████▍                 | 24.6 MB 9.3 MB/s eta 0:00:049.3 MB/s eta 0:00:04▏              | 29.4 MB 9.3 MB/s eta 0:00:03   | 32.6 MB 9.3 MB/s eta 0:00:03    |████████████████████▎           | 34.7 MB 9.3 MB/s eta 0:00:03    | 37.5 MB 9.3 MB/s eta 0:00:02��█████████████████████▉        | 40.7 MB 9.3 MB/s eta 0:00:02�██████████████████▎      | 43.1 MB 9.3 MB/s eta 0:00:02��█████████████████▊     | 45.5 MB 7.1 MB/s eta 0:00:02��██████████  | 51.1 MB 7.1 MB/s eta 0:00:01█████████████████████████▋| 53.9 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/filelock/3.12.0/filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting sympy\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/sympy/1.12/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     |████████████████████████████████| 5.7 MB 8.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch) (2.8.8)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cusparse-cu11/11.7.4.91/nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "     |████████████████████████████████| 173.2 MB 91 kB/s                          | 9.8 MB 12.5 MB/s eta 0:00:14                     | 14.9 MB 12.5 MB/s eta 0:00:132.5 MB/s eta 0:00:12                   | 30.3 MB 12.5 MB/s eta 0:00:12MB/s eta 0:00:12MB/s eta 0:00:12MB/s eta 0:00:112.5 MB/s eta 0:00:11MB 12.5 MB/s eta 0:00:11MB 12.5 MB/s eta 0:00:11��████▏                      | 49.8 MB 42.2 MB/s eta 0:00:03                   | 52.6 MB 42.2 MB/s eta 0:00:03 42.2 MB/s eta 0:00:03                 | 57.7 MB 42.2 MB/s eta 0:00:03��████▎                    | 61.2 MB 42.2 MB/s eta 0:00:03��████▊                    | 63.7 MB 42.2 MB/s eta 0:00:03MB/s eta 0:00:03 | 70.8 MB 42.2 MB/s eta 0:00:03██▋                  | 73.4 MB 42.2 MB/s eta 0:00:032 MB/s eta 0:00:03██████████▍                | 83.4 MB 42.2 MB/s eta 0:00:03         | 86.0 MB 42.2 MB/s eta 0:00:03�██████████▍               | 88.7 MB 42.2 MB/s eta 0:00:03/s eta 0:00:02/s eta 0:00:02  | 109.1 MB 38.3 MB/s eta 0:00:02  | 112.3 MB 38.3 MB/s eta 0:00:02�████████▎          | 115.1 MB 38.3 MB/s eta 0:00:02�████████▋          | 117.0 MB 38.3 MB/s eta 0:00:02��████████████████████▏         | 119.9 MB 38.3 MB/s eta 0:00:02B 38.3 MB/s eta 0:00:02B 38.3 MB/s eta 0:00:02█████▍       | 132.3 MB 38.3 MB/s eta 0:00:02█████████████████████████       | 135.0 MB 38.3 MB/s eta 0:00:01��████████████████▍      | 137.6 MB 38.3 MB/s eta 0:00:01██████▉      | 139.8 MB 38.3 MB/s eta 0:00:01| 149.1 MB 36.4 MB/s eta 0:00:01| 152.0 MB 36.4 MB/s eta 0:00:01��████████████████████▏  | 157.7 MB 36.4 MB/s eta 0:00:019 MB 36.4 MB/s eta 0:00:01��██████████████████████▏ | 163.0 MB 36.4 MB/s eta 0:00:01 |██████████████████████████████▋ | 165.4 MB 36.4 MB/s eta 0:00:01 |███████████████████████████████ | 167.6 MB 36.4 MB/s eta 0:00:01█████▊| 172.0 MB 36.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-nccl-cu11/2.14.3/nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "     |████████████████████████████████| 177.1 MB 129 kB/s             4.5 MB 6.2 MB/s eta 0:00:28MB/s eta 0:00:280 MB 6.2 MB/s eta 0:00:2626 0:00:26                      | 23.0 MB 6.2 MB/s eta 0:00:25                        | 25.9 MB 6.2 MB/s eta 0:00:25��█▏                          | 28.3 MB 6.2 MB/s eta 0:00:240 MB 6.2 MB/s eta 0:00:24 MB/s eta 0:00:23��█████▉                         | 38.0 MB 6.2 MB/s eta 0:00:23 |███████▌                        | 41.2 MB 6.2 MB/s eta 0:00:22 0:00:03MB 45.3 MB/s eta 0:00:03                   | 51.1 MB 45.3 MB/s eta 0:00:03��████▉                      | 54.5 MB 45.3 MB/s eta 0:00:03MB 45.3 MB/s eta 0:00:03MB 45.3 MB/s eta 0:00:03��████▍                    | 63.3 MB 45.3 MB/s eta 0:00:03           | 65.9 MB 45.3 MB/s eta 0:00:03MB/s eta 0:00:03MB/s eta 0:00:03██▌                  | 74.5 MB 45.3 MB/s eta 0:00:03  |█████████████▉                  | 76.7 MB 45.3 MB/s eta 0:00:03         | 86.5 MB 45.3 MB/s eta 0:00:02 | 89.8 MB 45.3 MB/s eta 0:00:02�██████████▉               | 93.0 MB 45.3 MB/s eta 0:00:02/s eta 0:00:02��█▏             | 100.8 MB 5.7 MB/s eta 0:00:14��█▉             | 104.1 MB 5.7 MB/s eta 0:00:13████████▎            | 106.4 MB 5.7 MB/s eta 0:00:13████████▊            | 109.0 MB 5.7 MB/s eta 0:00:12MB/s eta 0:00:12MB/s eta 0:00:11��██▊          | 120.4 MB 5.7 MB/s eta 0:00:10███████████████▎         | 123.0 MB 5.7 MB/s eta 0:00:10███████████████▋         | 125.3 MB 5.7 MB/s eta 0:00:10████████████████         | 127.8 MB 5.7 MB/s eta 0:00:09:00:09B 5.7 MB/s eta 0:00:08�███████████▏      | 139.3 MB 5.7 MB/s eta 0:00:07�███████████▊      | 142.1 MB 5.7 MB/s eta 0:00:07    |██████████████████████████▎     | 145.2 MB 56.1 MB/s eta 0:00:01�████████▏    | 150.4 MB 56.1 MB/s eta 0:00:01| 152.0 MB 56.1 MB/s eta 0:00:01| 154.8 MB 56.1 MB/s eta 0:00:01��███████████████████▎  | 162.0 MB 56.1 MB/s eta 0:00:01��████████████████████  | 165.4 MB 56.1 MB/s eta 0:00:01 |██████████████████████████████▎ | 167.8 MB 56.1 MB/s eta 0:00:01�███████████████████████████▉ | 170.6 MB 56.1 MB/s eta 0:00:01 MB 56.1 MB/s eta 0:00:01 MB 56.1 MB/s eta 0:00:01 MB 56.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cublas-cu11/11.10.3.66/nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "     |████████████████████████████████| 317.1 MB 26.1 MB/s            4.0 MB 6.8 MB/s eta 0:00:47              | 19.1 MB 6.8 MB/s eta 0:00:440.6 MB 13.7 MB/s eta 0:00:21                   | 50.9 MB 13.7 MB/s eta 0:00:207 MB/s eta 0:00:20                   | 56.8 MB 13.7 MB/s eta 0:00:20                   | 59.3 MB 13.7 MB/s eta 0:00:19MB/s eta 0:00:19MB/s eta 0:00:19MB/s eta 0:00:19MB 13.7 MB/s eta 0:00:19██▊                        | 76.2 MB 17.1 MB/s eta 0:00:15MB 17.1 MB/s eta 0:00:14█████▋                       | 85.0 MB 17.1 MB/s eta 0:00:14MB 17.1 MB/s eta 0:00:14██▏                      | 90.5 MB 17.1 MB/s eta 0:00:14��████▍                      | 93.3 MB 17.1 MB/s eta 0:00:14��████▊                      | 95.8 MB 17.1 MB/s eta 0:00:13��█████                      | 98.9 MB 17.1 MB/s eta 0:00:13              | 101.8 MB 17.1 MB/s eta 0:00:13              | 106.1 MB 17.1 MB/s eta 0:00:13             | 108.6 MB 17.1 MB/s eta 0:00:13 |███████████▏                    | 111.0 MB 17.1 MB/s eta 0:00:13██████████▍                    | 113.2 MB 17.1 MB/s eta 0:00:12 |███████████▊                    | 116.5 MB 17.1 MB/s eta 0:00:12 |████████████                    | 119.3 MB 46.6 MB/s eta 0:00:05███████████▎                   | 122.2 MB 46.6 MB/s eta 0:00:05                   | 124.7 MB 46.6 MB/s eta 0:00:05                   | 126.9 MB 46.6 MB/s eta 0:00:05�█████████                   | 129.4 MB 46.6 MB/s eta 0:00:05.6 MB/s eta 0:00:04��████████▋                  | 134.8 MB 46.6 MB/s eta 0:00:04��████████▉                  | 137.4 MB 46.6 MB/s eta 0:00:046.6 MB/s eta 0:00:04��██▎                 | 142.0 MB 46.6 MB/s eta 0:00:046.6 MB/s eta 0:00:046.6 MB/s eta 0:00:04███▏                | 150.0 MB 46.6 MB/s eta 0:00:04███▍                | 152.8 MB 46.6 MB/s eta 0:00:04███▋                | 155.2 MB 46.6 MB/s eta 0:00:04��████████▉                | 157.3 MB 46.6 MB/s eta 0:00:04�███████▏               | 159.7 MB 46.6 MB/s eta 0:00:04 |████████████████▎               | 161.8 MB 46.6 MB/s eta 0:00:04███████████████▌               | 163.9 MB 5.9 MB/s eta 0:00:26  |████████████████▉               | 166.4 MB 5.9 MB/s eta 0:00:26  |█████████████████               | 169.5 MB 5.9 MB/s eta 0:00:25  | 172.0 MB 5.9 MB/s eta 0:00:25 ��█████▋              | 174.7 MB 5.9 MB/s eta 0:00:25  | 177.2 MB 5.9 MB/s eta 0:00:24 ��███████▏             | 180.2 MB 5.9 MB/s eta 0:00:24 ��███████▌             | 183.0 MB 5.9 MB/s eta 0:00:23 ██████▊             | 185.7 MB 5.9 MB/s eta 0:00:23 ��████████             | 188.4 MB 5.9 MB/s eta 0:00:22     |███████████████████▎            | 190.6 MB 5.9 MB/s eta 0:00:22 ��██████████████████▍            | 192.3 MB 5.9 MB/s eta 0:00:22     |███████████████████▉            | 196.9 MB 5.9 MB/s eta 0:00:21   | 201.7 MB 5.9 MB/s eta 0:00:20 ��██████████████████▌           | 203.5 MB 5.9 MB/s eta 0:00:20   | 205.9 MB 40.2 MB/s eta 0:00:03��███████████████████           | 208.4 MB 40.2 MB/s eta 0:00:03�████████▎          | 211.1 MB 40.2 MB/s eta 0:00:03�████████▌          | 213.5 MB 40.2 MB/s eta 0:00:03�█████████          | 219.1 MB 40.2 MB/s eta 0:00:03B 40.2 MB/s eta 0:00:03█████▎       | 240.9 MB 20.2 MB/s eta 0:00:04█████▊       | 245.4 MB 20.2 MB/s eta 0:00:04██████       | 247.1 MB 20.2 MB/s eta 0:00:04██████▏      | 249.8 MB 20.2 MB/s eta 0:00:04��████████████████▌      | 252.2 MB 20.2 MB/s eta 0:00:04��████████████████▋      | 254.1 MB 20.2 MB/s eta 0:00:04��█████████████████      | 258.4 MB 20.2 MB/s eta 0:00:03| 270.7 MB 20.2 MB/s eta 0:00:03| 273.3 MB 40.3 MB/s eta 0:00:02| 275.8 MB 40.3 MB/s eta 0:00:02████████████████████    | 278.1 MB 40.3 MB/s eta 0:00:01��████████▋   | 283.6 MB 40.3 MB/s eta 0:00:01��█████████   | 286.3 MB 40.3 MB/s eta 0:00:01███████████████████████▏  | 289.4 MB 40.3 MB/s eta 0:00:01��███████████████████▍  | 291.6 MB 40.3 MB/s eta 0:00:01��███████████████████▋  | 293.8 MB 40.3 MB/s eta 0:00:01��████████████████████  | 296.5 MB 40.3 MB/s eta 0:00:01�███████████████████████████▏ | 299.1 MB 40.3 MB/s eta 0:00:01 |██████████████████████████████▍ | 301.1 MB 40.3 MB/s eta 0:00:01 |██████████████████████████████▌ | 302.4 MB 40.3 MB/s eta 0:00:01█████████████████████████████▉ | 305.1 MB 40.3 MB/s eta 0:00:01�████████████████████████████ | 307.1 MB 40.3 MB/s eta 0:00:01 MB 40.3 MB/s eta 0:00:01 MB 40.3 MB/s eta 0:00:01 MB 40.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cufft-cu11/10.9.0.58/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "     |████████████████████████████████| 168.4 MB 51.1 MB/s            s eta 0:00:411 MB 4.0 MB/s eta 0:00:409 MB 4.0 MB/s eta 0:00:3939                        | 21.9 MB 4.0 MB/s eta 0:00:38                        | 25.4 MB 4.0 MB/s eta 0:00:37                   | 28.0 MB 59.9 MB/s eta 0:00:03                   | 31.1 MB 59.9 MB/s eta 0:00:03 | 33.5 MB 59.9 MB/s eta 0:00:03MB/s eta 0:00:039.9 MB/s eta 0:00:03███                        | 41.7 MB 59.9 MB/s eta 0:00:03MB 59.9 MB/s eta 0:00:03                   | 49.2 MB 59.9 MB/s eta 0:00:02��█████                      | 52.1 MB 59.9 MB/s eta 0:00:02MB 59.9 MB/s eta 0:00:02��████▍                    | 60.1 MB 59.9 MB/s eta 0:00:02MB/s eta 0:00:02MB/s eta 0:00:02MB/s eta 0:00:02███                  | 73.2 MB 59.9 MB/s eta 0:00:02         | 80.9 MB 86.9 MB/s eta 0:00:02         | 83.0 MB 86.9 MB/s eta 0:00:01�██████████▊               | 87.8 MB 86.9 MB/s eta 0:00:01�███████████               | 89.7 MB 86.9 MB/s eta 0:00:01/s eta 0:00:01/s eta 0:00:01�▌             | 97.3 MB 86.9 MB/s eta 0:00:01��██             | 99.8 MB 86.9 MB/s eta 0:00:01    |███████████████████▉            | 104.5 MB 86.9 MB/s eta 0:00:01  | 107.1 MB 86.9 MB/s eta 0:00:01  | 109.7 MB 86.9 MB/s eta 0:00:01�████████▎          | 112.2 MB 86.9 MB/s eta 0:00:01�████████▊          | 114.4 MB 86.9 MB/s eta 0:00:01��████████████████████▎         | 117.1 MB 86.9 MB/s eta 0:00:01��████████████████████▊         | 119.6 MB 98.7 MB/s eta 0:00:01B 98.7 MB/s eta 0:00:01B 98.7 MB/s eta 0:00:01B 98.7 MB/s eta 0:00:01��██▌       | 129.1 MB 98.7 MB/s eta 0:00:01█████▉       | 130.7 MB 98.7 MB/s eta 0:00:01��████████████████▊      | 135.3 MB 98.7 MB/s eta 0:00:01| 144.7 MB 98.7 MB/s eta 0:00:01| 147.3 MB 98.7 MB/s eta 0:00:01��████████▍   | 149.4 MB 98.7 MB/s eta 0:00:01████████▌  | 155.2 MB 98.7 MB/s eta 0:00:01█████████  | 158.3 MB 98.7 MB/s eta 0:00:01�███████████████████████████▌ | 160.6 MB 51.1 MB/s eta 0:00:01 |███████████████████████████████ | 163.2 MB 51.1 MB/s eta 0:00:01 MB 51.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cusolver-cu11/11.4.0.1/nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "     |████████████████████████████████| 102.6 MB 5.1 MB/s             █▏                        | 23.1 MB 11.7 MB/s eta 0:00:07                   | 35.7 MB 11.7 MB/s eta 0:00:06███████████                    | 38.3 MB 110.9 MB/s eta 0:00:01��██████▋                   | 40.4 MB 110.9 MB/s eta 0:00:01��████████▌                  | 43.2 MB 110.9 MB/s eta 0:00:010.9 MB/s eta 0:00:01███▎                | 48.8 MB 110.9 MB/s eta 0:00:01��█████████                | 51.0 MB 110.9 MB/s eta 0:00:01 |████████████████▋               | 53.4 MB 110.9 MB/s eta 0:00:01 | 56.1 MB 110.9 MB/s eta 0:00:01██████████████▏             | 58.2 MB 110.9 MB/s eta 0:00:01��████████             | 61.1 MB 110.9 MB/s eta 0:00:01    |███████████████████▉            | 63.6 MB 110.9 MB/s eta 0:00:01�████████▏          | 68.0 MB 110.9 MB/s eta 0:00:01�█████████          | 70.4 MB 110.9 MB/s eta 0:00:01█████████████████▌         | 72.3 MB 110.9 MB/s eta 0:00:01 110.9 MB/s eta 0:00:01 110.9 MB/s eta 0:00:01��████████████████▉      | 82.7 MB 5.1 MB/s eta 0:00:04  | 88.3 MB 5.1 MB/s eta 0:00:03  ��████████▍   | 91.0 MB 5.1 MB/s eta 0:00:03  ██████████████████████████▏  | 93.6 MB 5.1 MB/s eta 0:00:02  ��███████████████████▉  | 95.5 MB 5.1 MB/s eta 0:00:02   |██████████████████████████████▊ | 98.6 MB 5.1 MB/s eta 0:00:01   MB 5.1 MB/s eta 0:00:01 \n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/nvidia-cuda-cupti-cu11/11.7.101/nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "     |████████████████████████████████| 11.8 MB 5.3 MB/s             MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.6.0)\n",
      "Collecting lit\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/lit/16.0.5/lit-16.0.5.tar.gz (138 kB)\n",
      "     |████████████████████████████████| 138 kB 12.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/cmake/3.26.3/cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "     |████████████████████████████████| 24.0 MB 6.4 MB/s            �█████████████████▉     | 20.1 MB 6.4 MB/s eta 0:00:01B/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading https://repo-mirror.mos.ru/repository/pypi-proxy/packages/mpmath/1.3.0/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     |████████████████████████████████| 536 kB 16.0 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.5-py3-none-any.whl size=88176 sha256=3c86d697019b32ddd2f41caf81f14628f2f3663dd8cc07663edd031b37085204\n",
      "  Stored in directory: /home/maslovapo_user/.cache/pip/wheels/e3/84/4a/10d1b522e6b4372c78b1882d860d33f71ea8612fa08dad1108\n",
      "Successfully built lit\n",
      "Installing collected packages: nvidia-cublas-cu11, mpmath, lit, filelock, cmake, triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-cusolver-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, torch\n",
      "Successfully installed cmake-3.26.3 filelock-3.12.0 lit-16.0.5 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276c4624-30da-49c9-994b-c5fe960ab4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189262e7-ce40-404a-becd-07410008aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1):\n",
    "        print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "        for line in tqdm.tqdm(texts):\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819ad51d-b208-481e-8e75-bb3b88b3867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    def __init__(self, options):\n",
    "        self.model_path = options[\"model_path\"]\n",
    "        self.vocab_path = options[\"vocab_path\"]\n",
    "        self.device = options[\"device\"]\n",
    "        self.window_size = options[\"window_size\"]\n",
    "        self.adaptive_window = options[\"adaptive_window\"]\n",
    "        self.seq_len = options[\"seq_len\"]\n",
    "        self.corpus_lines = options[\"corpus_lines\"]\n",
    "        self.on_memory = options[\"on_memory\"]\n",
    "        self.batch_size = options[\"batch_size\"]\n",
    "        self.num_workers = options[\"num_workers\"]\n",
    "        self.num_candidates = options[\"num_candidates\"]\n",
    "        self.output_dir = options[\"output_dir\"]\n",
    "        self.model_dir = options[\"model_dir\"]\n",
    "        self.gaussian_mean = options[\"gaussian_mean\"]\n",
    "        self.gaussian_std = options[\"gaussian_std\"]\n",
    "\n",
    "        self.is_logkey = options[\"is_logkey\"]\n",
    "        self.is_time = options[\"is_time\"]\n",
    "        self.scale_path = options[\"scale_path\"]\n",
    "\n",
    "        self.hypersphere_loss = options[\"hypersphere_loss\"]\n",
    "        self.hypersphere_loss_test = options[\"hypersphere_loss_test\"]\n",
    "\n",
    "        self.lower_bound = self.gaussian_mean - 3 * self.gaussian_std\n",
    "        self.upper_bound = self.gaussian_mean + 3 * self.gaussian_std\n",
    "\n",
    "        self.center = None\n",
    "        self.radius = None\n",
    "        self.test_ratio = options[\"test_ratio\"]\n",
    "        self.mask_ratio = options[\"mask_ratio\"]\n",
    "        self.min_len=options[\"min_len\"]\n",
    "\n",
    "    def detect_logkey_anomaly(self, masked_output, masked_label):\n",
    "        num_undetected_tokens = 0\n",
    "        output_maskes = []\n",
    "        for i, token in enumerate(masked_label):\n",
    "            # output_maskes.append(torch.argsort(-masked_output[i])[:30].cpu().numpy()) # extract top 30 candidates for mask labels\n",
    "\n",
    "            if token not in torch.argsort(-masked_output[i])[:self.num_candidates]:\n",
    "                num_undetected_tokens += 1\n",
    "\n",
    "        return num_undetected_tokens, [output_maskes, masked_label.cpu().numpy()]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_test(output_dir, file_name, window_size, adaptive_window, seq_len, scale, min_len):\n",
    "        \"\"\"\n",
    "        :return: log_seqs: num_samples x session(seq)_length, tim_seqs: num_samples x session_length\n",
    "        \"\"\"\n",
    "        log_seqs = []\n",
    "        tim_seqs = []\n",
    "        with open(output_dir + file_name, \"r\") as f:\n",
    "            for idx, line in tqdm(enumerate(f.readlines())):\n",
    "                #if idx > 40: break\n",
    "                log_seq, tim_seq = fixed_window(line, window_size,\n",
    "                                                adaptive_window=adaptive_window,\n",
    "                                                seq_len=seq_len, min_len=min_len)\n",
    "                if len(log_seq) == 0:\n",
    "                    continue\n",
    "\n",
    "                # if scale is not None:\n",
    "                #     times = tim_seq\n",
    "                #     for i, tn in enumerate(times):\n",
    "                #         tn = np.array(tn).reshape(-1, 1)\n",
    "                #         times[i] = scale.transform(tn).reshape(-1).tolist()\n",
    "                #     tim_seq = times\n",
    "\n",
    "                log_seqs += log_seq\n",
    "                tim_seqs += tim_seq\n",
    "\n",
    "        # sort seq_pairs by seq len\n",
    "        log_seqs = np.array(log_seqs)\n",
    "        tim_seqs = np.array(tim_seqs)\n",
    "\n",
    "        test_len = list(map(len, log_seqs))\n",
    "        test_sort_index = np.argsort(-1 * np.array(test_len))\n",
    "\n",
    "        log_seqs = log_seqs[test_sort_index]\n",
    "        tim_seqs = tim_seqs[test_sort_index]\n",
    "\n",
    "        print(f\"{file_name} size: {len(log_seqs)}\")\n",
    "        return log_seqs, tim_seqs\n",
    "\n",
    "    def helper(self, model, output_dir, file_name, vocab, scale=None, error_dict=None):\n",
    "        total_results = []\n",
    "        total_errors = []\n",
    "        output_results = []\n",
    "        total_dist = []\n",
    "        output_cls = []\n",
    "        logkey_test, time_test = self.generate_test(output_dir, file_name, self.window_size, self.adaptive_window, self.seq_len, scale, self.min_len)\n",
    "\n",
    "        # use 1/10 test data\n",
    "        if self.test_ratio != 1:\n",
    "            num_test = len(logkey_test)\n",
    "            rand_index = torch.randperm(num_test)\n",
    "            rand_index = rand_index[:int(num_test * self.test_ratio)] if isinstance(self.test_ratio, float) else rand_index[:self.test_ratio]\n",
    "            logkey_test, time_test = logkey_test[rand_index], time_test[rand_index]\n",
    "\n",
    "\n",
    "        seq_dataset = LogDataset(logkey_test, time_test, vocab, seq_len=self.seq_len,\n",
    "                                 corpus_lines=self.corpus_lines, on_memory=self.on_memory, predict_mode=True, mask_ratio=self.mask_ratio)\n",
    "\n",
    "        # use large batch size in test data\n",
    "        data_loader = DataLoader(seq_dataset, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                                 collate_fn=seq_dataset.collate_fn)\n",
    "\n",
    "        for idx, data in enumerate(data_loader):\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            result = model(data[\"bert_input\"], data[\"time_input\"])\n",
    "\n",
    "            # mask_lm_output, mask_tm_output: batch_size x session_size x vocab_size\n",
    "            # cls_output: batch_size x hidden_size\n",
    "            # bert_label, time_label: batch_size x session_size\n",
    "            # in session, some logkeys are masked\n",
    "\n",
    "            mask_lm_output, mask_tm_output = result[\"logkey_output\"], result[\"time_output\"]\n",
    "            output_cls += result[\"cls_output\"].tolist()\n",
    "\n",
    "            # dist = torch.sum((result[\"cls_output\"] - self.hyper_center) ** 2, dim=1)\n",
    "            # when visualization no mask\n",
    "            # continue\n",
    "\n",
    "            # loop though each session in batch\n",
    "            for i in range(len(data[\"bert_label\"])):\n",
    "                seq_results = {\"num_error\": 0,\n",
    "                               \"undetected_tokens\": 0,\n",
    "                               \"masked_tokens\": 0,\n",
    "                               \"total_logkey\": torch.sum(data[\"bert_input\"][i] > 0).item(),\n",
    "                               \"deepSVDD_label\": 0\n",
    "                               }\n",
    "\n",
    "                mask_index = data[\"bert_label\"][i] > 0\n",
    "                num_masked = torch.sum(mask_index).tolist()\n",
    "                seq_results[\"masked_tokens\"] = num_masked\n",
    "\n",
    "                if self.is_logkey:\n",
    "                    num_undetected, output_seq = self.detect_logkey_anomaly(\n",
    "                        mask_lm_output[i][mask_index], data[\"bert_label\"][i][mask_index])\n",
    "                    seq_results[\"undetected_tokens\"] = num_undetected\n",
    "\n",
    "                    output_results.append(output_seq)\n",
    "\n",
    "                if self.hypersphere_loss_test:\n",
    "                    # detect by deepSVDD distance\n",
    "                    assert result[\"cls_output\"][i].size() == self.center.size()\n",
    "                    # dist = torch.sum((result[\"cls_fnn_output\"][i] - self.center) ** 2)\n",
    "                    dist = torch.sqrt(torch.sum((result[\"cls_output\"][i] - self.center) ** 2))\n",
    "                    total_dist.append(dist.item())\n",
    "\n",
    "                    # user defined threshold for deepSVDD_label\n",
    "                    seq_results[\"deepSVDD_label\"] = int(dist.item() > self.radius)\n",
    "                    #\n",
    "                    # if dist > 0.25:\n",
    "                    #     pass\n",
    "\n",
    "                if idx < 10 or idx % 1000 == 0:\n",
    "                    print(\n",
    "                        \"{}, #time anomaly: {} # of undetected_tokens: {}, # of masked_tokens: {} , \"\n",
    "                        \"# of total logkey {}, deepSVDD_label: {} \\n\".format(\n",
    "                            file_name,\n",
    "                            seq_results[\"num_error\"],\n",
    "                            seq_results[\"undetected_tokens\"],\n",
    "                            seq_results[\"masked_tokens\"],\n",
    "                            seq_results[\"total_logkey\"],\n",
    "                            seq_results['deepSVDD_label']\n",
    "                        )\n",
    "                    )\n",
    "                total_results.append(seq_results)\n",
    "\n",
    "        # for time\n",
    "        # return total_results, total_errors\n",
    "\n",
    "        #for logkey\n",
    "        # return total_results, output_results\n",
    "\n",
    "        # for hypersphere distance\n",
    "        return total_results, output_cls\n",
    "\n",
    "    def predict(self):\n",
    "        model = torch.load(self.model_path)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        print('model_path: {}'.format(self.model_path))\n",
    "\n",
    "        start_time = time.time()\n",
    "        vocab = WordVocab.load_vocab(self.vocab_path)\n",
    "\n",
    "        scale = None\n",
    "        error_dict = None\n",
    "        if self.is_time:\n",
    "            with open(self.scale_path, \"rb\") as f:\n",
    "                scale = pickle.load(f)\n",
    "\n",
    "            with open(self.model_dir + \"error_dict.pkl\", 'rb') as f:\n",
    "                error_dict = pickle.load(f)\n",
    "\n",
    "        if self.hypersphere_loss:\n",
    "            center_dict = torch.load(self.model_dir + \"best_center.pt\")\n",
    "            self.center = center_dict[\"center\"]\n",
    "            self.radius = center_dict[\"radius\"]\n",
    "            # self.center = self.center.view(1,-1)\n",
    "\n",
    "\n",
    "        print(\"test normal predicting\")\n",
    "        test_normal_results, test_normal_errors = self.helper(model, self.output_dir, \"test_normal\", vocab, scale, error_dict)\n",
    "\n",
    "        print(\"test abnormal predicting\")\n",
    "        test_abnormal_results, test_abnormal_errors = self.helper(model, self.output_dir, \"test_abnormal\", vocab, scale, error_dict)\n",
    "\n",
    "        print(\"Saving test normal results\")\n",
    "        with open(self.model_dir + \"test_normal_results\", \"wb\") as f:\n",
    "            pickle.dump(test_normal_results, f)\n",
    "\n",
    "        print(\"Saving test abnormal results\")\n",
    "        with open(self.model_dir + \"test_abnormal_results\", \"wb\") as f:\n",
    "            pickle.dump(test_abnormal_results, f)\n",
    "\n",
    "        print(\"Saving test normal errors\")\n",
    "        with open(self.model_dir + \"test_normal_errors.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_normal_errors, f)\n",
    "\n",
    "        print(\"Saving test abnormal results\")\n",
    "        with open(self.model_dir + \"test_abnormal_errors.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_abnormal_errors, f)\n",
    "\n",
    "        params = {\"is_logkey\": self.is_logkey, \"is_time\": self.is_time, \"hypersphere_loss\": self.hypersphere_loss,\n",
    "                  \"hypersphere_loss_test\": self.hypersphere_loss_test}\n",
    "        best_th, best_seq_th, FP, TP, TN, FN, P, R, F1 = find_best_threshold(test_normal_results,\n",
    "                                                                            test_abnormal_results,\n",
    "                                                                            params=params,\n",
    "                                                                            th_range=np.arange(10),\n",
    "                                                                            seq_range=np.arange(0,1,0.1))\n",
    "\n",
    "        print(\"best threshold: {}, best threshold ratio: {}\".format(best_th, best_seq_th))\n",
    "        print(\"TP: {}, TN: {}, FP: {}, FN: {}\".format(TP, TN, FP, FN))\n",
    "        print('Precision: {:.2f}%, Recall: {:.2f}%, F1-measure: {:.2f}%'.format(P, R, F1))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('elapsed_time: {}'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d5c1cb-be27-4af7-a793-4bfe4276783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    def __init__(self, options):\n",
    "        self.model_path = options[\"model_path\"]\n",
    "        self.vocab_path = options[\"vocab_path\"]\n",
    "        self.device = options[\"device\"]\n",
    "        self.window_size = options[\"window_size\"]\n",
    "        self.adaptive_window = options[\"adaptive_window\"]\n",
    "        self.seq_len = options[\"seq_len\"]\n",
    "        self.corpus_lines = options[\"corpus_lines\"]\n",
    "        self.on_memory = options[\"on_memory\"]\n",
    "        self.batch_size = options[\"batch_size\"]\n",
    "        self.num_workers = options[\"num_workers\"]\n",
    "        self.num_candidates = options[\"num_candidates\"]\n",
    "        self.output_dir = options[\"output_dir\"]\n",
    "        self.model_dir = options[\"model_dir\"]\n",
    "        self.gaussian_mean = options[\"gaussian_mean\"]\n",
    "        self.gaussian_std = options[\"gaussian_std\"]\n",
    "\n",
    "        self.is_logkey = options[\"is_logkey\"]\n",
    "        self.is_time = options[\"is_time\"]\n",
    "        self.scale_path = options[\"scale_path\"]\n",
    "\n",
    "        self.hypersphere_loss = options[\"hypersphere_loss\"]\n",
    "        self.hypersphere_loss_test = options[\"hypersphere_loss_test\"]\n",
    "\n",
    "        self.lower_bound = self.gaussian_mean - 3 * self.gaussian_std\n",
    "        self.upper_bound = self.gaussian_mean + 3 * self.gaussian_std\n",
    "\n",
    "        self.center = None\n",
    "        self.radius = None\n",
    "        self.test_ratio = options[\"test_ratio\"]\n",
    "        self.mask_ratio = options[\"mask_ratio\"]\n",
    "        self.min_len=options[\"min_len\"]\n",
    "\n",
    "    def detect_logkey_anomaly(self, masked_output, masked_label):\n",
    "        num_undetected_tokens = 0\n",
    "        output_maskes = []\n",
    "        for i, token in enumerate(masked_label):\n",
    "            # output_maskes.append(torch.argsort(-masked_output[i])[:30].cpu().numpy()) # extract top 30 candidates for mask labels\n",
    "\n",
    "            if token not in torch.argsort(-masked_output[i])[:self.num_candidates]:\n",
    "                num_undetected_tokens += 1\n",
    "\n",
    "        return num_undetected_tokens, [output_maskes, masked_label.cpu().numpy()]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_test(output_dir, file_name, window_size, adaptive_window, seq_len, scale, min_len):\n",
    "        \"\"\"\n",
    "        :return: log_seqs: num_samples x session(seq)_length, tim_seqs: num_samples x session_length\n",
    "        \"\"\"\n",
    "        log_seqs = []\n",
    "        tim_seqs = []\n",
    "        with open(output_dir + file_name, \"r\") as f:\n",
    "            for idx, line in tqdm(enumerate(f.readlines())):\n",
    "                #if idx > 40: break\n",
    "                log_seq, tim_seq = fixed_window(line, window_size,\n",
    "                                                adaptive_window=adaptive_window,\n",
    "                                                seq_len=seq_len, min_len=min_len)\n",
    "                if len(log_seq) == 0:\n",
    "                    continue\n",
    "\n",
    "                # if scale is not None:\n",
    "                #     times = tim_seq\n",
    "                #     for i, tn in enumerate(times):\n",
    "                #         tn = np.array(tn).reshape(-1, 1)\n",
    "                #         times[i] = scale.transform(tn).reshape(-1).tolist()\n",
    "                #     tim_seq = times\n",
    "\n",
    "                log_seqs += log_seq\n",
    "                tim_seqs += tim_seq\n",
    "\n",
    "        # sort seq_pairs by seq len\n",
    "        log_seqs = np.array(log_seqs)\n",
    "        tim_seqs = np.array(tim_seqs)\n",
    "\n",
    "        test_len = list(map(len, log_seqs))\n",
    "        test_sort_index = np.argsort(-1 * np.array(test_len))\n",
    "\n",
    "        log_seqs = log_seqs[test_sort_index]\n",
    "        tim_seqs = tim_seqs[test_sort_index]\n",
    "\n",
    "        print(f\"{file_name} size: {len(log_seqs)}\")\n",
    "        return log_seqs, tim_seqs\n",
    "\n",
    "    def helper(self, model, output_dir, file_name, vocab, scale=None, error_dict=None):\n",
    "        total_results = []\n",
    "        total_errors = []\n",
    "        output_results = []\n",
    "        total_dist = []\n",
    "        output_cls = []\n",
    "        logkey_test, time_test = self.generate_test(output_dir, file_name, self.window_size, self.adaptive_window, self.seq_len, scale, self.min_len)\n",
    "\n",
    "        # use 1/10 test data\n",
    "        if self.test_ratio != 1:\n",
    "            num_test = len(logkey_test)\n",
    "            rand_index = torch.randperm(num_test)\n",
    "            rand_index = rand_index[:int(num_test * self.test_ratio)] if isinstance(self.test_ratio, float) else rand_index[:self.test_ratio]\n",
    "            logkey_test, time_test = logkey_test[rand_index], time_test[rand_index]\n",
    "\n",
    "\n",
    "        seq_dataset = LogDataset(logkey_test, time_test, vocab, seq_len=self.seq_len,\n",
    "                                 corpus_lines=self.corpus_lines, on_memory=self.on_memory, predict_mode=True, mask_ratio=self.mask_ratio)\n",
    "\n",
    "        # use large batch size in test data\n",
    "        data_loader = DataLoader(seq_dataset, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                                 collate_fn=seq_dataset.collate_fn)\n",
    "\n",
    "        for idx, data in enumerate(data_loader):\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            result = model(data[\"bert_input\"], data[\"time_input\"])\n",
    "\n",
    "            # mask_lm_output, mask_tm_output: batch_size x session_size x vocab_size\n",
    "            # cls_output: batch_size x hidden_size\n",
    "            # bert_label, time_label: batch_size x session_size\n",
    "            # in session, some logkeys are masked\n",
    "\n",
    "            mask_lm_output, mask_tm_output = result[\"logkey_output\"], result[\"time_output\"]\n",
    "            output_cls += result[\"cls_output\"].tolist()\n",
    "\n",
    "            # dist = torch.sum((result[\"cls_output\"] - self.hyper_center) ** 2, dim=1)\n",
    "            # when visualization no mask\n",
    "            # continue\n",
    "\n",
    "            # loop though each session in batch\n",
    "            for i in range(len(data[\"bert_label\"])):\n",
    "                seq_results = {\"num_error\": 0,\n",
    "                               \"undetected_tokens\": 0,\n",
    "                               \"masked_tokens\": 0,\n",
    "                               \"total_logkey\": torch.sum(data[\"bert_input\"][i] > 0).item(),\n",
    "                               \"deepSVDD_label\": 0\n",
    "                               }\n",
    "\n",
    "                mask_index = data[\"bert_label\"][i] > 0\n",
    "                num_masked = torch.sum(mask_index).tolist()\n",
    "                seq_results[\"masked_tokens\"] = num_masked\n",
    "\n",
    "                if self.is_logkey:\n",
    "                    num_undetected, output_seq = self.detect_logkey_anomaly(\n",
    "                        mask_lm_output[i][mask_index], data[\"bert_label\"][i][mask_index])\n",
    "                    seq_results[\"undetected_tokens\"] = num_undetected\n",
    "\n",
    "                    output_results.append(output_seq)\n",
    "\n",
    "                if self.hypersphere_loss_test:\n",
    "                    # detect by deepSVDD distance\n",
    "                    assert result[\"cls_output\"][i].size() == self.center.size()\n",
    "                    # dist = torch.sum((result[\"cls_fnn_output\"][i] - self.center) ** 2)\n",
    "                    dist = torch.sqrt(torch.sum((result[\"cls_output\"][i] - self.center) ** 2))\n",
    "                    total_dist.append(dist.item())\n",
    "\n",
    "                    # user defined threshold for deepSVDD_label\n",
    "                    seq_results[\"deepSVDD_label\"] = int(dist.item() > self.radius)\n",
    "                    #\n",
    "                    # if dist > 0.25:\n",
    "                    #     pass\n",
    "\n",
    "                if idx < 10 or idx % 1000 == 0:\n",
    "                    print(\n",
    "                        \"{}, #time anomaly: {} # of undetected_tokens: {}, # of masked_tokens: {} , \"\n",
    "                        \"# of total logkey {}, deepSVDD_label: {} \\n\".format(\n",
    "                            file_name,\n",
    "                            seq_results[\"num_error\"],\n",
    "                            seq_results[\"undetected_tokens\"],\n",
    "                            seq_results[\"masked_tokens\"],\n",
    "                            seq_results[\"total_logkey\"],\n",
    "                            seq_results['deepSVDD_label']\n",
    "                        )\n",
    "                    )\n",
    "                total_results.append(seq_results)\n",
    "\n",
    "        # for time\n",
    "        # return total_results, total_errors\n",
    "\n",
    "        #for logkey\n",
    "        # return total_results, output_results\n",
    "\n",
    "        # for hypersphere distance\n",
    "        return total_results, output_cls\n",
    "\n",
    "    def predict(self):\n",
    "        model = torch.load(self.model_path)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        print('model_path: {}'.format(self.model_path))\n",
    "\n",
    "        start_time = time.time()\n",
    "        vocab = WordVocab.load_vocab(self.vocab_path)\n",
    "\n",
    "        scale = None\n",
    "        error_dict = None\n",
    "        if self.is_time:\n",
    "            with open(self.scale_path, \"rb\") as f:\n",
    "                scale = pickle.load(f)\n",
    "\n",
    "            with open(self.model_dir + \"error_dict.pkl\", 'rb') as f:\n",
    "                error_dict = pickle.load(f)\n",
    "\n",
    "        if self.hypersphere_loss:\n",
    "            center_dict = torch.load(self.model_dir + \"best_center.pt\")\n",
    "            self.center = center_dict[\"center\"]\n",
    "            self.radius = center_dict[\"radius\"]\n",
    "            # self.center = self.center.view(1,-1)\n",
    "\n",
    "\n",
    "        print(\"test normal predicting\")\n",
    "        test_normal_results, test_normal_errors = self.helper(model, self.output_dir, \"test_normal\", vocab, scale, error_dict)\n",
    "\n",
    "        print(\"test abnormal predicting\")\n",
    "        test_abnormal_results, test_abnormal_errors = self.helper(model, self.output_dir, \"test_abnormal\", vocab, scale, error_dict)\n",
    "\n",
    "        print(\"Saving test normal results\")\n",
    "        with open(self.model_dir + \"test_normal_results\", \"wb\") as f:\n",
    "            pickle.dump(test_normal_results, f)\n",
    "\n",
    "        print(\"Saving test abnormal results\")\n",
    "        with open(self.model_dir + \"test_abnormal_results\", \"wb\") as f:\n",
    "            pickle.dump(test_abnormal_results, f)\n",
    "\n",
    "        print(\"Saving test normal errors\")\n",
    "        with open(self.model_dir + \"test_normal_errors.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_normal_errors, f)\n",
    "\n",
    "        print(\"Saving test abnormal results\")\n",
    "        with open(self.model_dir + \"test_abnormal_errors.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_abnormal_errors, f)\n",
    "\n",
    "        params = {\"is_logkey\": self.is_logkey, \"is_time\": self.is_time, \"hypersphere_loss\": self.hypersphere_loss,\n",
    "                  \"hypersphere_loss_test\": self.hypersphere_loss_test}\n",
    "        best_th, best_seq_th, FP, TP, TN, FN, P, R, F1 = find_best_threshold(test_normal_results,\n",
    "                                                                            test_abnormal_results,\n",
    "                                                                            params=params,\n",
    "                                                                            th_range=np.arange(10),\n",
    "                                                                            seq_range=np.arange(0,1,0.1))\n",
    "\n",
    "        print(\"best threshold: {}, best threshold ratio: {}\".format(best_th, best_seq_th))\n",
    "        print(\"TP: {}, TN: {}, FP: {}, FN: {}\".format(TP, TN, FP, FN))\n",
    "        print('Precision: {:.2f}%, Recall: {:.2f}%, F1-measure: {:.2f}%'.format(P, R, F1))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('elapsed_time: {}'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2cc770-e669-4a06-aa1d-b2207ee0d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/KirillVladimirov/005ec7f762293d2321385580d3dbe335\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261de50-fb91-4759-97d9-7bee02e34f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05774560-7958-45c3-a913-7ccb31fb98bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] {train,predict,vocab} ...\n",
      "ipykernel_launcher.py: error: argument {train,predict,vocab}: invalid choice: '/home/maslovapo_user/.local/share/jupyter/runtime/kernel-b391786e-3c44-4d06-867d-91b97e2c8b94.json' (choose from 'train', 'predict', 'vocab')\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n",
      "features logkey:True time: False\n",
      "\n",
      "mask ratio 0.65\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 1853, in parse_known_args\n",
      "    namespace, args = self._parse_known_args(args, namespace)\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 2065, in _parse_known_args\n",
      "    stop_index = consume_positionals(start_index)\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 2021, in consume_positionals\n",
      "    take_action(action, args)\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 1914, in take_action\n",
      "    argument_values = self._get_values(action, argument_strings)\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 2455, in _get_values\n",
      "    self._check_value(action, value[0])\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 2502, in _check_value\n",
      "    raise ArgumentError(action, msg % args)\n",
      "argparse.ArgumentError: argument {train,predict,vocab}: invalid choice: '/home/maslovapo_user/.local/share/jupyter/runtime/kernel-b391786e-3c44-4d06-867d-91b97e2c8b94.json' (choose from 'train', 'predict', 'vocab')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_191/1417529885.py\", line 98, in <module>\n",
      "    args = parser.parse_args()\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 1820, in parse_args\n",
      "    args, argv = self.parse_known_args(args, namespace)\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 1856, in parse_known_args\n",
      "    self.error(str(err))\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 2577, in error\n",
      "    self.exit(2, _('%(prog)s: error: %(message)s\\n') % args)\n",
      "  File \"/opt/conda/lib/python3.9/argparse.py\", line 2564, in exit\n",
      "    _sys.exit(status)\n",
      "SystemExit: 2\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1852\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m                 \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;31m# consume any positionals following the last Optional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m         \u001b[0mstop_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsume_positionals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36mconsume_positionals\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   2020\u001b[0m                 \u001b[0mstart_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0marg_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m                 \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(action, argument_strings, option_string)\u001b[0m\n\u001b[1;32m   1913\u001b[0m             \u001b[0mseen_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m             \u001b[0margument_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(self, action, arg_strings)\u001b[0m\n\u001b[1;32m   2454\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_strings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36m_check_value\u001b[0;34m(self, action, value)\u001b[0m\n\u001b[1;32m   2501\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid choice: %(value)r (choose from %(choices)s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2502\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument {train,predict,vocab}: invalid choice: '/home/maslovapo_user/.local/share/jupyter/runtime/kernel-b391786e-3c44-4d06-867d-91b97e2c8b94.json' (choose from 'train', 'predict', 'vocab')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_191/1417529885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arguments\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1820\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1855\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2577\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2563\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2055\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2056\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2057\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2058\u001b[0m                                                                      value))\n\u001b[1;32m   2059\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "dirname = os.path.dirname(os.path.abspath('__file__'))\n",
    "filename = os.path.join(dirname, '../deeplog')\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "#from bert_pytorch.dataset import WordVocab\n",
    "#from bert_pytorch import Predictor, Trainer\n",
    "#from bert_pytorch.dataset.utils import seed_everything\n",
    "\n",
    "options = dict()\n",
    "options['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "options[\"output_dir\"] = \"\"\n",
    "options[\"model_dir\"] = options[\"output_dir\"] + \"bert/\"\n",
    "options[\"model_path\"] = options[\"model_dir\"] + \"best_bert.pth\"\n",
    "options[\"train_vocab\"] = options[\"output_dir\"] + \"train\"\n",
    "options[\"vocab_path\"] = options[\"output_dir\"] + \"vocab.pkl\"  # pickle file\n",
    "\n",
    "options[\"window_size\"] = 128\n",
    "options[\"adaptive_window\"] = True\n",
    "options[\"seq_len\"] = 512\n",
    "options[\"max_len\"] = 512 # for position embedding\n",
    "options[\"min_len\"] = 10\n",
    "options[\"mask_ratio\"] = 0.65\n",
    "# sample ratio\n",
    "options[\"train_ratio\"] = 1\n",
    "options[\"valid_ratio\"] = 0.1\n",
    "options[\"test_ratio\"] = 1\n",
    "\n",
    "# features\n",
    "options[\"is_logkey\"] = True\n",
    "options[\"is_time\"] = False\n",
    "\n",
    "options[\"hypersphere_loss\"] = True\n",
    "options[\"hypersphere_loss_test\"] = False\n",
    "\n",
    "options[\"scale\"] = None # MinMaxScaler()\n",
    "options[\"scale_path\"] = options[\"model_dir\"] + \"scale.pkl\"\n",
    "\n",
    "# model\n",
    "options[\"hidden\"] = 256 # embedding size\n",
    "options[\"layers\"] = 4\n",
    "options[\"attn_heads\"] = 4\n",
    "\n",
    "options[\"epochs\"] = 200\n",
    "options[\"n_epochs_stop\"] = 10\n",
    "options[\"batch_size\"] = 32\n",
    "\n",
    "options[\"corpus_lines\"] = None\n",
    "options[\"on_memory\"] = True\n",
    "options[\"num_workers\"] = 5\n",
    "options[\"lr\"] = 1e-3\n",
    "options[\"adam_beta1\"] = 0.9\n",
    "options[\"adam_beta2\"] = 0.999\n",
    "options[\"adam_weight_decay\"] = 0.00\n",
    "options[\"with_cuda\"]= True\n",
    "options[\"cuda_devices\"] = None\n",
    "options[\"log_freq\"] = None\n",
    "\n",
    "# predict\n",
    "options[\"num_candidates\"] = 6\n",
    "options[\"gaussian_mean\"] = 0\n",
    "options[\"gaussian_std\"] = 1\n",
    "\n",
    "seed_everything(seed=1234)\n",
    "\n",
    "if not os.path.exists(options['model_dir']):\n",
    "    os.makedirs(options['model_dir'], exist_ok=True)\n",
    "\n",
    "print(\"device\", options[\"device\"])\n",
    "print(\"features logkey:{} time: {}\\n\".format(options[\"is_logkey\"], options[\"is_time\"]))\n",
    "print(\"mask ratio\", options[\"mask_ratio\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    train_parser = subparsers.add_parser('train')\n",
    "    train_parser.set_defaults(mode='train')\n",
    "\n",
    "    predict_parser = subparsers.add_parser('predict')\n",
    "    predict_parser.set_defaults(mode='predict')\n",
    "    predict_parser.add_argument(\"-m\", \"--mean\", type=float, default=0)\n",
    "    predict_parser.add_argument(\"-s\", \"--std\", type=float, default=1)\n",
    "\n",
    "    vocab_parser = subparsers.add_parser('vocab')\n",
    "    vocab_parser.set_defaults(mode='vocab')\n",
    "    vocab_parser.add_argument(\"-s\", \"--vocab_size\", type=int, default=None)\n",
    "    vocab_parser.add_argument(\"-e\", \"--encoding\", type=str, default=\"utf-8\")\n",
    "    vocab_parser.add_argument(\"-m\", \"--min_freq\", type=int, default=1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"arguments\", args)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        Trainer(options).train()\n",
    "\n",
    "    elif args.mode == 'predict':\n",
    "        Predictor(options).predict()\n",
    "\n",
    "    elif args.mode == 'vocab':\n",
    "        with open(options[\"train_vocab\"], \"r\", encoding=args.encoding) as f:\n",
    "            texts = f.readlines()\n",
    "        vocab = WordVocab(texts, max_size=args.vocab_size, min_freq=args.min_freq)\n",
    "        print(\"VOCAB SIZE:\", len(vocab))\n",
    "        print(\"save vocab in\", options[\"vocab_path\"])\n",
    "        vocab.save_vocab(options[\"vocab_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd2f25-a388-4de5-a665-c1dbd4bcb4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
